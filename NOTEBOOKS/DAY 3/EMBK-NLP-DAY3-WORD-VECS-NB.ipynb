{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors\n",
    "Word vectors refers to a family of related techniques, first gaining popularity via ```Word2Vec``` which associates an $n$-dimensional (normally $n$ is in the range of $50$ to $500$.  For us it will be $300$) vector to every word in the target language.  While the reasoning is different for every different method of creating word vectors, the goal of all of these techniques is to associate vectors which are close to one another to words that have similar meaning.  As we will see as we explore further, they will see they actually provide significantly more!\n",
    "\n",
    "We will first load a batch of word vectors known as [ConceptNet Numberbatch](https://github.com/commonsense/conceptnet-numberbatch), which have been found to have excellent performance while reducing issues of [learning human bias](https://gist.github.com/rspeer/ef750e7e407e04894cb3b78a82d66aed).  Learning how to construct these word vectors is a bit beyond the scope of what we can cover in this notebook, but [this two-part blog post provides an excellent introduction](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/).\n",
    "\n",
    "## How to get the most from this notebook\n",
    "This builds out a solution in a step by step manner making it clear where data is being used and what tools are useful for exploration.  After every code block, I encourage you to explore your own problem from beginning to end using these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-12 03:39:41--  https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz\n",
      "Resolving conceptnet.s3.amazonaws.com (conceptnet.s3.amazonaws.com)... 52.216.168.219\n",
      "Connecting to conceptnet.s3.amazonaws.com (conceptnet.s3.amazonaws.com)|52.216.168.219|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 269500348 (257M) [text/plain]\n",
      "Saving to: ‘numberbatch-en-17.06.txt.gz’\n",
      "\n",
      "numberbatch-en-17.0 100%[===================>] 257.01M  61.5MB/s    in 4.3s    \n",
      "\n",
      "2020-02-12 03:39:46 (60.0 MB/s) - ‘numberbatch-en-17.06.txt.gz’ saved [269500348/269500348]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gzip -d numberbatch-en-17.06.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load word vectors\n",
    "words = pd.read_csv('numberbatch-en-17.06.txt', sep=\" \", index_col=0, header=None, skiprows=[0]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet above loads our wordvectors.  The Pandas table ```words``` allows us to perform lookups like ```words['test']``` to get the associated vectors.  Let's just print one out for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      0.1232\n",
      "2     -0.0096\n",
      "3     -0.0189\n",
      "4     -0.1528\n",
      "5     -0.0639\n",
      "6     -0.0365\n",
      "7      0.0946\n",
      "8     -0.0558\n",
      "9     -0.1471\n",
      "10    -0.0069\n",
      "11    -0.0103\n",
      "12    -0.0385\n",
      "13    -0.0127\n",
      "14     0.0326\n",
      "15    -0.0225\n",
      "16    -0.0232\n",
      "17    -0.0497\n",
      "18     0.0140\n",
      "19     0.0426\n",
      "20    -0.0174\n",
      "21     0.0781\n",
      "22    -0.0574\n",
      "23    -0.0185\n",
      "24    -0.0658\n",
      "25    -0.0429\n",
      "26    -0.0232\n",
      "27    -0.1080\n",
      "28     0.0140\n",
      "29    -0.0731\n",
      "30     0.0466\n",
      "        ...  \n",
      "271   -0.0030\n",
      "272   -0.0092\n",
      "273    0.0086\n",
      "274    0.0389\n",
      "275    0.0024\n",
      "276   -0.0363\n",
      "277    0.0069\n",
      "278    0.0089\n",
      "279    0.0344\n",
      "280    0.0686\n",
      "281   -0.0064\n",
      "282    0.0398\n",
      "283   -0.0224\n",
      "284   -0.0315\n",
      "285    0.0248\n",
      "286    0.0047\n",
      "287   -0.0326\n",
      "288   -0.0316\n",
      "289   -0.0431\n",
      "290    0.0200\n",
      "291   -0.0535\n",
      "292   -0.0381\n",
      "293   -0.0409\n",
      "294   -0.0373\n",
      "295    0.0541\n",
      "296   -0.0098\n",
      "297    0.0182\n",
      "298   -0.0073\n",
      "299    0.0321\n",
      "300    0.0814\n",
      "Name: test, Length: 300, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(words['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a vector is not particularly informative to us since it is not organized in a humanly readable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Distances\n",
    "As part of our \"manipulation primitives\", we often need to be able to compute distances between vectors associated to words.  So we start by writing a little snippet that lets us do so.  ```numpy``` makes this fairly easy to do.  Remember that small distances correspond to similar words, so lets check this by going through and writing a little code that takes three words and tells you if the first word is closer to the second than the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the distance between two words\n",
    "def dist(w1,w2) :\n",
    "    return np.linalg.norm(words[w1] - words[w2])\n",
    "    \n",
    "# Say if w1 is closer to w2 than w3\n",
    "def distCompare(w1, w2, w3) :\n",
    "    d2 = dist(w1,w2)\n",
    "    d3 = dist(w1,w3)\n",
    "    if d2 < d3 :\n",
    "        print(\"{} is closer to {} than {}\".format(w1,w2,w3))\n",
    "    else :\n",
    "        print(\"{} is closer to {} than {}\".format(w1,w3,w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orb is closer to ball than hockey\n",
      "picked is closer to lifted than play\n",
      "pink is closer to red than blue\n"
     ]
    }
   ],
   "source": [
    "distCompare('orb','ball','hockey')\n",
    "distCompare('picked','lifted','play')\n",
    "distCompare('pink','red','blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it mostly agrees with what we anticipated.  If you continued to ask more questions, you'd find some things that disagree with what you would expect (for instance, it believes that ```'maroon'``` is closer to ```'blue'``` than ```'red'```), but on the whole, you'll find it agrees with the intuition that similar words should be close to one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Structure of Word Vectors (Subtraction)\n",
    "\n",
    "If word vectors only put similar words next to one another, they would have never garnered the interest that they have obtained from the community.  Indeed they actually contain subtle and nuanced understanding of the meanings of words.  It will take a while to explore what this means, but the mantra that we should now internalize is \"relationships = directions\" which is to say that words that share a similar relationship, will be separated from one another in the same direction.\n",
    "\n",
    "As we saw, vector subtraction lets us examine this.  However, since vector subtraction is as simple as\n",
    "```python\n",
    "diff = v - w\n",
    "```\n",
    "there is not much to look at here.\n",
    "\n",
    "## Reverse Lookup\n",
    "\n",
    "Reverse lookup will allow us to probe the finer structure of word vectors.  In particular, we will now create a reverse lookup routine that finds the $k$ closest words to a given vector.  As the straight-forward implementation will be too slow (looping over every element of ```words```) we will provide you with a ```numpy``` implementation which will be fast enough for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue' 'bluecurls' 'bluishness' 'bluer' 'unblue' 'bluest' 'blueish'\n",
      " 'cyanol' 'bluely' 'gridelin' 'iridovirus' 'ceruleous' 'roygbiv'\n",
      " 'cyanophore' 'lazuline' 'berylline' 'acyanopia' 'bluing' 'blueness'\n",
      " 'chromostereopsis' 'umangite' 'red' 'yellowred' 'bluet' 'bluetit'\n",
      " 'purple' 'yellow' 'kumst' 'cerulean' 'purpre' 'mauvette' 'bepurple'\n",
      " 'bluish' 'purpureal' 'pyrrh' 'deredden' 'indigoidine' 'zaffre' 'argb'\n",
      " 'bloncket' 'turquoisey' 'puniceous' 'luteo' 'rubiform' 'xanthous'\n",
      " 'xanthochromic' 'crustaceorubin' 'tetronerythrin' 'vitellorubin'\n",
      " 'cyanophyll']\n"
     ]
    }
   ],
   "source": [
    "# drop things containing underscores (these are compound terms like \"young_man\" that our code will not use) and convert to matrix format for faster computation\n",
    "labels  = words.columns.values.tolist()\n",
    "labels = np.array([w for w in labels if isinstance(w,str) and w.isalpha()])\n",
    "wordsMatrix = words[labels].as_matrix()\n",
    "\n",
    "# snipped to find the closest word (or vector)\n",
    "def find_closest_word(v, k = 1):\n",
    "    if type(v) == type('str'):\n",
    "        v = words[v]\n",
    "    diff = wordsMatrix - v.values.reshape(-1,1)\n",
    "    delta = np.linalg.norm(diff, axis=0)\n",
    "    return labels[np.argsort(delta)[:k]]\n",
    "    \n",
    "# test with the 50 closest words to blue\n",
    "print(find_closest_word('blue', 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work to me!  Lots of blue related words, and then words related to other colors.  Many of them, like ```'tetronerythrin'``` actually relate to specific pigments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Analogies\n",
    "Let's dive in to an application of these primitives now: solving analogies.  This is the first and best way to see the idea that \"relationship = direction,\" and was really the \"killer app\" for word vectors.  Consider the analogy that ```man:woman::boy:girl``` which is read \"man is to woman as boy is to girl.\"  What such an analogy means is that the relationship between ```'man'``` and ```'woman'``` is the same as between ```'boy'``` and ```'girl'```, in particular the relationship of gender. \n",
    "\n",
    "Let us suppose that we have the four associated word vectors $v_{man}$, $v_{woman}$, $v_{boy}$, and $v_{girl}$.  If we believe the idea that \"relationship = direction\" then, this becomes a vector relationship, where the vector that takes us from $v_{man}$ to $v_{woman}$ should be the same as the vector that takes us from $v_{boy}$ to $v_{girl}$.  Recalling that vector subtraction is what gives us such a direction, this becomes\n",
    "$$\n",
    "v_{woman} - v_{man} \\approx v_{girl} - v_{boy}\n",
    "$$\n",
    "\n",
    "Suppose you now wanted to solve an analogy using this idea.  Say we were just given ```man:woman::boy:?``` and we wanted to find the question mark.  The expression above can be rearranged by adding $v_{boy}$ to both sides to yield\n",
    "$$\n",
    "v_{?} \\approx v_{woman} - v_{man} + v_{boy}\n",
    "$$\n",
    "Thus the word we are looking for should hopefully be the word whose associated vector is closest to $v_{woman} - v_{man} + v_{boy}$.  Let's see how this works out in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little snippet for analogies\n",
    "def analogy(w1,w2,w3, k = 1) : \n",
    "    listPoss = find_closest_word(words[w2] - words[w1] + words[w3], k)\n",
    "    print(\"{} : {} :: {} : {}\".format(w1,w2,w3,listPoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : woman :: boy : ['girl' 'boy' 'kinderwhore' 'faunlet']\n",
      "short : tall :: shortest : ['tallest' 'tall' 'tallish' 'procere']\n",
      "seattle : washington :: minneapolis : ['minnesota' 'minneapolis' 'minneapolitan' 'mankato']\n"
     ]
    }
   ],
   "source": [
    "# A few examples\n",
    "analogy('man','woman','boy', 4)\n",
    "analogy('short','tall','shortest', 4)\n",
    "analogy('seattle','washington','minneapolis', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, this should have worked fantastically!  The first one will indeed show that ```man : woman :: boy : girl``` as the most likely choice. The second will state that ```short : tall :: shortest : tallest``` is the most likely case indicating that it understands how to turn words into superlatives (not just a simple relationship of size).  The third one indicates it understands what the largest cities in Washington and Minnesota are and can retrieve that information if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization (PCA)\n",
    "\n",
    "Let's implement our last little primitive: the ability to automatically visualize what a collection of vectors is doing by projecting it onto the best possible pair of directions.  We will use [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pcaPlot(word_list) :\n",
    "    # fetch list of word vectors\n",
    "    vecs = [words[x] for x in word_list]\n",
    "    \n",
    "    #reduce dimensions\n",
    "    model = PCA(n_components = 2)\n",
    "    reduced = model.fit_transform(vecs)\n",
    "    xc = [v[0] for v in reduced]\n",
    "    yc = [v[1] for v in reduced]\n",
    "    \n",
    "    # plot them\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(xc, yc)\n",
    "\n",
    "    # label the plot\n",
    "    for i, word in enumerate(word_list) :\n",
    "    \tplt.annotate(word, xy=(xc[i], yc[i]+0.01), fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "pcaPlot(['fast','faster','fastest','slow','slower','slowest'])\n",
    "pcaPlot(['bird', 'cat', 'squirrel', 'dog', 'fish', 'helicopter', 'airplane', 'car', 'submarine', 'whale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting notebook artifacts\n",
    "! rm -rf numberbatch-en-17.06.txt.gz\n",
    "! rm numberbatch-en-17.06.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
