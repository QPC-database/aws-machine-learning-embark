{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs) for the Regression Problem\n",
    "\n",
    "In this notebook, we build a Recurrent Neural Networks (RNNs) using GloVe word embeddings, to predict the __log_votes__ field of our review dataset.\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __rating:__ Rating of the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "\n",
    "__Important note:__ One big distinction betweeen the regular neural networks and RNNs is that RNNs work with sequential data. In our case, RNNs will help us with the text field. If we also want to consider other fields such as time, rating, verified, etc. , we need to use the regular neural networks and connect it to the RNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "! pip install -q gluonnlp mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "from mxnet.gluon import nn, rnn, Trainer\n",
    "from mxnet.gluon.loss import L2Loss \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset below and fill-in the reviewText field. We will use this field as input to our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../DATA/NLP/EMBK-NLP-REVIEW-DATA-CSV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five rows in the dataset. As you can see the __log_votes__ field is numeric. That's why we will build a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>log_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuck with this at work, slow and we still got...</td>\n",
       "      <td>Use SEP or Mcafee</td>\n",
       "      <td>False</td>\n",
       "      <td>1464739200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I use parallels every day with both my persona...</td>\n",
       "      <td>Use it daily</td>\n",
       "      <td>False</td>\n",
       "      <td>1332892800</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barbara Robbins\\n\\nI've used TurboTax to do ou...</td>\n",
       "      <td>Helpful Product</td>\n",
       "      <td>True</td>\n",
       "      <td>1398816000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have been using this software security for y...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1430784000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you want your computer hijacked and slowed ...</td>\n",
       "      <td>... hijacked and slowed to a crawl Windows 10 ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1508025600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  Stuck with this at work, slow and we still got...   \n",
       "1  I use parallels every day with both my persona...   \n",
       "2  Barbara Robbins\\n\\nI've used TurboTax to do ou...   \n",
       "3  I have been using this software security for y...   \n",
       "4  If you want your computer hijacked and slowed ...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                                  Use SEP or Mcafee     False  1464739200   \n",
       "1                                       Use it daily     False  1332892800   \n",
       "2                                    Helpful Product      True  1398816000   \n",
       "3                                         Five Stars      True  1430784000   \n",
       "4  ... hijacked and slowed to a crawl Windows 10 ...     False  1508025600   \n",
       "\n",
       "   rating  log_votes  \n",
       "0     1.0        0.0  \n",
       "1     5.0        0.0  \n",
       "2     4.0        0.0  \n",
       "3     5.0        0.0  \n",
       "4     1.0        0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis and Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the range and distribution of log_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"log_votes\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.799753318287247"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"log_votes\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"log_votes\"].plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewText    6\n",
      "summary       7\n",
      "verified      0\n",
      "time          0\n",
      "rating        0\n",
      "log_votes     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider the reviewText field. Let's fill-in the missing values for that below. We will just use the placeholder \"Missing\" here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviewText\"].fillna(\"Missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train-validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This separates 15% of the entire dataset into validation dataset.\n",
    "train_text, val_text, train_label, val_label = \\\n",
    "    train_test_split(df[\"reviewText\"].tolist(),\n",
    "                     df[\"log_votes\"].tolist(),\n",
    "                     test_size=0.15,\n",
    "                     random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text processing and Transformation\n",
    "We will apply the following processes here:\n",
    "* __Text cleaning:__ Simple text cleaning operations. We won't do stemming or lemmatization as our word vectors already cover different forms of words. We are using GloVe word embeddings for 6 billion words, phrases or punctuations in this example.\n",
    "* __Tokenization:__ Tokenizing all sentences\n",
    "* __Creating vocabulary:__ We will create a vocabulary of the tokens. In this vocabulary, tokens will map to unique ids, such as \"car\"->32, \"house\"->651, etc.\n",
    "* __Transforming text:__ Tokenized sentences will be mapped to unique ids. For example: [\"this\", \"is\", \"sentence\"] -> [13, 54, 412]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    }
   ],
   "source": [
    "import nltk, gluonnlp\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def cleanStr(text):\n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.lower().strip()\n",
    "    # Remove extra space and tabs\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    # Remove HTML tags/markups\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    text = cleanStr(text)\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def createVocabulary(text_list, min_freq):\n",
    "    all_tokens = []\n",
    "    for sentence in text_list:\n",
    "        all_tokens += tokenize(sentence)\n",
    "    # Calculate token frequencies\n",
    "    counter = gluonnlp.data.count_tokens(all_tokens)\n",
    "    # Create the vocabulary\n",
    "    vocab = gluonnlp.Vocab(counter,\n",
    "                           min_freq = min_freq,\n",
    "                           unknown_token = '<unk>',\n",
    "                           padding_token = None,\n",
    "                           bos_token = None,\n",
    "                           eos_token = None)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def transformText(text, vocab, max_length):\n",
    "    token_arr = np.zeros((max_length,))\n",
    "    tokens = tokenize(text)[0:max_length]\n",
    "    for idx, token in enumerate(tokens):\n",
    "        try:\n",
    "            # Use the vocabulary index of the token\n",
    "            token_arr[idx] = vocab.token_to_idx[token]\n",
    "        except:\n",
    "            token_arr[idx] = 0 # Unknown word\n",
    "    return token_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep the training time low, we only consider the first 250 words (max_length) in sentences. We also only use words that occur more than 5 times in the all sentences (min_freq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary\n",
      "Transforming training texts\n",
      "Transforming validation texts\n"
     ]
    }
   ],
   "source": [
    "min_freq = 5\n",
    "max_length = 250\n",
    "\n",
    "print(\"Creating the vocabulary\")\n",
    "vocab = createVocabulary(train_text, min_freq)\n",
    "print(\"Transforming training texts\")\n",
    "train_text_transformed = nd.array([transformText(text, vocab, max_length) for text in train_text])\n",
    "print(\"Transforming validation texts\")\n",
    "val_text_transformed = nd.array([transformText(text, vocab, max_length) for text in val_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some unique ids for some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary index for computer: 67\n",
      "Vocabulary index for beautiful: 2033\n",
      "Vocabulary index for code: 421\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary index for computer:\", vocab['computer'])\n",
    "print(\"Vocabulary index for beautiful:\", vocab['beautiful'])\n",
    "print(\"Vocabulary index for code:\", vocab['code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Using pre-trained GloVe Word Embeddings:\n",
    "\n",
    "In this example, we will use GloVe word vectors. `'glove.6B.50d.txt'` file gives us 6 billion words/phrases vectors. Each word vector has 50 numbers in it. The following code shows how to get the word vectors and create an embedding matrix from them. We will connect our vocabulary indexes to the GloVe embedding with the `get_vecs_by_tokens()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/ec2-user/.mxnet/embeddings/glove/glove.6B.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/embeddings/glove/glove.6B.zip...\n"
     ]
    }
   ],
   "source": [
    "from mxnet.contrib import text\n",
    "glove = text.embedding.create('glove',\n",
    "                              pretrained_file_name = 'glove.6B.50d.txt')\n",
    "embedding_matrix = glove.get_vecs_by_tokens(vocab.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training and validation\n",
    "\n",
    "We have processed our text data and also created our embedding matrixes from GloVe. Now, it is time to start the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set our parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the state vectors\n",
    "hidden_size = 12\n",
    "\n",
    "# General NN training parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Embedding vector and vocabulary sizes\n",
    "num_embed = 50 # glove.6B.50d.txt\n",
    "vocab_size = len(vocab.token_to_idx.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to put our data into correct format before the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.data import ArrayDataset, DataLoader\n",
    "\n",
    "train_label = nd.array(train_label)\n",
    "val_label = nd.array(val_label)\n",
    "\n",
    "train_dataset = ArrayDataset(train_text_transformed, train_label)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sequential model is made of these layers:\n",
    "* Embedding layer: This is where our words/tokens are mapped to word vectors.\n",
    "* RNN layer: We will be using a simple RNN model. We won't stack RNN units in this example. It uses a sinle RNN unit with its hidden state size of 12. More details about the RNN is available [here](https://mxnet.incubator.apache.org/api/python/docs/api/gluon/rnn/index.html#mxnet.gluon.rnn.RNN).\n",
    "* Dense layer: A dense layer with a single neuron is used to output our log_votes prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.cpu()\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add(nn.Embedding(vocab_size, num_embed), # Embedding layer\n",
    "          rnn.RNN(hidden_size, num_layers=1),  # Recurrent layer\n",
    "          nn.Dense(1))                         # Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize this network. Then, we will need to make the embedding layer use our GloVe word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks parameters\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "# We set the embedding layer's parameters from GloVe\n",
    "model[0].weight.set_data(embedding_matrix.as_in_context(context))\n",
    "# We won't change/train the embedding layer\n",
    "model[0].collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the trainer and loss function below. L2 loss is used as this is a regression problem.\n",
    "$$\n",
    "\\mathrm{L2 loss} = \\frac{1}{2} \\sum_{i}(label_i - prediction_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting our trainer\n",
    "trainer = Trainer(model.collect_params(),\n",
    "                        'sgd',\n",
    "                        {'learning_rate': learning_rate})\n",
    "\n",
    "# We will use L2 loss\n",
    "l2_loss = L2Loss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to start the training process. We will print the L2 loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss 0.362745 Validation_loss 0.378338 Seconds 9.538171\n",
      "Epoch 1. Train_loss 0.343267 Validation_loss 0.377031 Seconds 9.300172\n",
      "Epoch 2. Train_loss 0.335926 Validation_loss 0.377808 Seconds 9.258245\n",
      "Epoch 3. Train_loss 0.330750 Validation_loss 0.379085 Seconds 9.378384\n",
      "Epoch 4. Train_loss 0.326565 Validation_loss 0.380351 Seconds 9.146582\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    # Training loop, train the network\n",
    "    for idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.as_in_context(context)\n",
    "        target = target.as_in_context(context)\n",
    "        \n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            L = l2_loss(output, target)\n",
    "            training_loss += nd.sum(L).asscalar()\n",
    "            L.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    val_predictions = model(val_text_transformed.as_in_context(context))\n",
    "    val_loss = nd.sum(l2_loss(val_predictions, val_label)).asscalar()\n",
    "    \n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(train_label)\n",
    "    val_loss = val_loss / len(val_label)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Epoch %s. Train_loss %f Validation_loss %f Seconds %f\" % \\\n",
    "          (epoch, training_loss, val_loss, end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
