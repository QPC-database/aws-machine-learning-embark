{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs) for the Product Review Problem - Classify Product Reviews as Positive or Not\n",
    "\n",
    "In this exercise, we will learn how to use Recurrent Neural Networks. \n",
    "\n",
    "We will follow these steps:\n",
    "1. <a href=\"#1\">Reading the dataset</a>\n",
    "2. <a href=\"#2\">Exploratory data analysis</a>\n",
    "3. <a href=\"#3\">Train-validation dataset split</a>\n",
    "4. <a href=\"#4\">Text processing and transformation</a>\n",
    "5. <a href=\"#5\">Using GloVe Word Embeddings</a>\n",
    "6. <a href=\"#6\">Training and validating model</a>\n",
    "7. <a href=\"#7\">Improvement ideas</a>\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "* __isPositive:__ Whether the review is positive or negative (1 or 0)\n",
    "\n",
    "__Important note:__ One big distinction betweeen the regular neural networks and RNNs is that RNNs work with sequential data. In our case, RNNs will help us with the text field. If we also want to consider other fields such as time, log_votes, verified, etc. , we need to use the regular neural networks with the RNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:47.162268Z",
     "start_time": "2021-01-09T05:02:47.160085Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q torch==1.8.1 torchtext nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:48.342987Z",
     "start_time": "2021-01-09T05:02:47.164823Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import BCEWithLogitsLoss \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Reading the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset below and fill-in the reviewText field. We will use this field as input to our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:48.995226Z",
     "start_time": "2021-01-09T05:02:48.344888Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../DATA/examples/AMAZON-REVIEW-DATA-CLASSIFICATION.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five rows in the dataset. As you can see the __log_votes__ field is numeric. That's why we will build a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:49.015545Z",
     "start_time": "2021-01-09T05:02:48.997444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>log_votes</th>\n",
       "      <th>isPositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PURCHASED FOR YOUNGSTER WHO\\nINHERITED MY \"TOO...</td>\n",
       "      <td>IDEAL FOR BEGINNER!</td>\n",
       "      <td>True</td>\n",
       "      <td>1361836800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unable to open or use</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1452643200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Waste of money!!! It wouldn't load to my system.</td>\n",
       "      <td>Dont buy it!</td>\n",
       "      <td>True</td>\n",
       "      <td>1433289600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I attempted to install this OS on two differen...</td>\n",
       "      <td>I attempted to install this OS on two differen...</td>\n",
       "      <td>True</td>\n",
       "      <td>1518912000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've spent 14 fruitless hours over the past tw...</td>\n",
       "      <td>Do NOT Download.</td>\n",
       "      <td>True</td>\n",
       "      <td>1441929600</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  PURCHASED FOR YOUNGSTER WHO\\nINHERITED MY \"TOO...   \n",
       "1                              unable to open or use   \n",
       "2   Waste of money!!! It wouldn't load to my system.   \n",
       "3  I attempted to install this OS on two differen...   \n",
       "4  I've spent 14 fruitless hours over the past tw...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                                IDEAL FOR BEGINNER!      True  1361836800   \n",
       "1                                          Two Stars      True  1452643200   \n",
       "2                                       Dont buy it!      True  1433289600   \n",
       "3  I attempted to install this OS on two differen...      True  1518912000   \n",
       "4                                   Do NOT Download.      True  1441929600   \n",
       "\n",
       "   log_votes  isPositive  \n",
       "0   0.000000         1.0  \n",
       "1   0.000000         0.0  \n",
       "2   0.000000         0.0  \n",
       "3   0.000000         0.0  \n",
       "4   1.098612         0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Exploratory Data Analysis</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the range and distribution of log_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:49.024615Z",
     "start_time": "2021-01-09T05:02:49.017492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    43692\n",
       "0.0    26308\n",
       "Name: isPositive, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"isPositive\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:49.040120Z",
     "start_time": "2021-01-09T05:02:49.026288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewText    11\n",
      "summary       14\n",
      "verified       0\n",
      "time           0\n",
      "log_votes      0\n",
      "isPositive     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have missing values in our text fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Train-validation split</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:49.098503Z",
     "start_time": "2021-01-09T05:02:49.041948Z"
    }
   },
   "outputs": [],
   "source": [
    "# This separates 15% of the entire dataset into validation dataset.\n",
    "train_text, val_text, train_label, val_label = \\\n",
    "    train_test_split(df[\"reviewText\"].tolist(),\n",
    "                     df[\"isPositive\"].tolist(),\n",
    "                     test_size=0.10,\n",
    "                     shuffle=True,\n",
    "                     random_state=324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Text processing and Transformation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will apply the following processes here:\n",
    "* __Text cleaning:__ Simple text cleaning operations. We won't do stemming or lemmatization as our word vectors already cover different forms of words. We are using GloVe word embeddings for 6 billion words, phrases or punctuations in this example.\n",
    "* __Tokenization:__ Tokenizing all sentences\n",
    "* __Creating vocabulary:__ We will create a vocabulary of the tokens. In this vocabulary, tokens will map to unique ids, such as \"car\"->32, \"house\"->651, etc.\n",
    "* __Transforming text:__ Tokenized sentences will be mapped to unique ids. For example: [\"this\", \"is\", \"sentence\"] -> [13, 54, 412]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:50.025716Z",
     "start_time": "2021-01-09T05:02:49.274150Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mimayer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk, torchtext\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def cleanStr(text):\n",
    "    \n",
    "    # Check if the sentence is a missing value\n",
    "    if isinstance(text, str) == False:\n",
    "        text = \"\"\n",
    "            \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.lower().strip()\n",
    "    # Remove extra space and tabs\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    # Remove HTML tags/markups\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    text = cleanStr(text)\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def createVocabulary(text_list, min_freq):\n",
    "    all_tokens = []\n",
    "    for sentence in text_list:\n",
    "        all_tokens += tokenize(sentence)\n",
    "    # Calculate token frequencies\n",
    "    counter = Counter()\n",
    "    for token in all_tokens:\n",
    "        counter[token] += 1\n",
    "    # Create the vocabulary\n",
    "    vocab = torchtext.vocab.Vocab(counter,\n",
    "                           min_freq = min_freq,\n",
    "                           specials = ('<unk>'))\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def transformText(text, vocab, max_length):\n",
    "    token_arr = torch.zeros((max_length,))\n",
    "    tokens = tokenize(text)[0:max_length]\n",
    "    for idx, token in enumerate(tokens):\n",
    "        try:\n",
    "            # Use the vocabulary index of the token\n",
    "            token_arr[idx] = vocab.stoi[token]\n",
    "        except:\n",
    "            token_arr[idx] = 0 # Unknown word\n",
    "    return token_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep the training time low, we only consider the first 250 words (max_length) in sentences. We also only use words that occur more than 5 times in the all sentences (min_freq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:03:26.376574Z",
     "start_time": "2021-01-09T05:02:50.027397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary\n"
     ]
    }
   ],
   "source": [
    "min_freq = 5\n",
    "max_length = 250\n",
    "\n",
    "print(\"Creating the vocabulary\")\n",
    "vocab = createVocabulary(train_text, min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.369651Z",
     "start_time": "2021-01-09T05:03:26.378501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming training texts\n",
      "Transforming validation texts\n"
     ]
    }
   ],
   "source": [
    "print(\"Transforming training texts\")\n",
    "train_text_transformed = torch.stack([transformText(text, vocab, max_length) for text in train_text])\n",
    "print(\"Transforming validation texts\")\n",
    "val_text_transformed = torch.stack([transformText(text, vocab, max_length) for text in val_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some unique ids for some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.374706Z",
     "start_time": "2021-01-09T05:04:29.371615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary index for computer: 71\n",
      "Vocabulary index for beautiful: 1935\n",
      "Vocabulary index for code: 407\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary index for computer:\", vocab['computer'])\n",
    "print(\"Vocabulary index for beautiful:\", vocab['beautiful'])\n",
    "print(\"Vocabulary index for code:\", vocab['code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Using pre-trained GloVe Word Embeddings</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this example, we will use GloVe word vectors. `name='6B'` `dim=50` gives us 6 billion words/phrases vectors. Each word vector has 50 numbers in it. The following code shows how to get the word vectors and create an embedding matrix from them. We will connect our vocabulary indexes to the GloVe embedding with the `get_vecs_by_tokens()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.864398Z",
     "start_time": "2021-01-09T05:04:29.376025Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:51, 5.04MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:07<00:00, 53606.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "glove = GloVe(name='6B', dim=50)\n",
    "embedding_matrix = glove.get_vecs_by_tokens(vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a name=\"6\">Training and validation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We have processed our text data and also created our embedding matrixes from GloVe. Now, it is time to start the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set our parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.868989Z",
     "start_time": "2021-01-09T05:04:29.866241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Size of the state vectors\n",
    "hidden_size = 12\n",
    "\n",
    "# General NN training parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "\n",
    "# Embedding vector and vocabulary sizes\n",
    "num_embed = 50 # glove.6B.50d.txt\n",
    "vocab_size = len(vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to put our data into correct format before the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.880059Z",
     "start_time": "2021-01-09T05:04:29.871107Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_label = torch.tensor(train_label)\n",
    "val_label = torch.tensor(val_label)\n",
    "train_dataset = TensorDataset(train_text_transformed, train_label)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is made of these layers:\n",
    "* Embedding layer: This is where our words/tokens are mapped to word vectors.\n",
    "* RNN layer: We will be using a simple RNN model. We won't stack RNN units in this example. It uses a sinle RNN unit with its hidden state size of 12. More details about the RNN is available [here](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html).\n",
    "* Linear layer: A linear layer with a single neuron is used to output our log_votes prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.892791Z",
     "start_time": "2021-01-09T05:04:29.881808Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") # use \"cuda:0\" if you are using GPU\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, num_hiddens, num_layers=num_layers)\n",
    "        self.linear = nn.Linear(3000, 1)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        outputs, _ = self.rnn(embeddings)\n",
    "        outs = self.linear(outputs.reshape(outputs.shape[0], -1))\n",
    "        return self.act(outs)\n",
    "\n",
    "model = Net(vocab_size, num_embed, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize this network. Then, we will need to make the embedding layer use our GloVe word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.902048Z",
     "start_time": "2021-01-09T05:04:29.899284Z"
    }
   },
   "outputs": [],
   "source": [
    "# We set the embedding layer's parameters from GloVe\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "# We won't change/train the embedding layer\n",
    "model.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the trainer and loss function below. __Binary cross-entropy loss__ is used as this is a binary classification problem.\n",
    "$$\n",
    "\\mathrm{BinaryCrossEntropyLoss} = -\\sum_{examples}{(y\\log(p) + (1 - y)\\log(1 - p))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.906415Z",
     "start_time": "2021-01-09T05:04:29.903716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting our trainer\n",
    "trainer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# We will use Binary Cross-entropy loss\n",
    "cross_ent_loss = nn.BCEWithLogitsLoss(reduction='none') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to start the training process. We will print the Binary cross-entropy loss loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:06:35.434926Z",
     "start_time": "2021-01-09T05:04:29.908071Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss 0.629892 Validation_loss 0.602221 Seconds 8.312113\n",
      "Epoch 1. Train_loss 0.598350 Validation_loss 0.582967 Seconds 7.471832\n",
      "Epoch 2. Train_loss 0.587359 Validation_loss 0.576832 Seconds 7.734207\n",
      "Epoch 3. Train_loss 0.579904 Validation_loss 0.574100 Seconds 7.834989\n",
      "Epoch 4. Train_loss 0.573832 Validation_loss 0.569988 Seconds 7.897461\n",
      "Epoch 5. Train_loss 0.569642 Validation_loss 0.570019 Seconds 7.486577\n",
      "Epoch 6. Train_loss 0.566562 Validation_loss 0.569189 Seconds 10.890178\n",
      "Epoch 7. Train_loss 0.563948 Validation_loss 0.573817 Seconds 11.268969\n",
      "Epoch 8. Train_loss 0.561591 Validation_loss 0.568954 Seconds 11.232005\n",
      "Epoch 9. Train_loss 0.559987 Validation_loss 0.568272 Seconds 10.575020\n",
      "Epoch 10. Train_loss 0.557994 Validation_loss 0.567034 Seconds 11.526398\n",
      "Epoch 11. Train_loss 0.556572 Validation_loss 0.568186 Seconds 11.837136\n",
      "Epoch 12. Train_loss 0.555396 Validation_loss 0.566539 Seconds 10.796005\n",
      "Epoch 13. Train_loss 0.554863 Validation_loss 0.563169 Seconds 10.581224\n",
      "Epoch 14. Train_loss 0.553484 Validation_loss 0.565254 Seconds 10.642124\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    # Training loop, train the network\n",
    "    for idx, (data, target) in enumerate(train_loader):\n",
    "        trainer.zero_grad()\n",
    "        data = data.long().to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        L = cross_ent_loss(output.squeeze(1), target).sum()\n",
    "        training_loss += L.item()\n",
    "        L.backward()\n",
    "        trainer.step()\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    val_predictions = model(val_text_transformed.long().to(device)).squeeze(1)\n",
    "    val_loss = cross_ent_loss(val_predictions, val_label).sum().item()\n",
    "    \n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(train_label)\n",
    "    val_loss = val_loss / len(val_label)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Epoch %s. Train_loss %f Validation_loss %f Seconds %f\" % \\\n",
    "          (epoch, training_loss, val_loss, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some validation results below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:06:36.046946Z",
     "start_time": "2021-01-09T05:06:35.436633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.74      0.70      2605\n",
      "         1.0       0.83      0.79      0.81      4395\n",
      "\n",
      "    accuracy                           0.77      7000\n",
      "   macro avg       0.75      0.76      0.76      7000\n",
      "weighted avg       0.77      0.77      0.77      7000\n",
      "\n",
      "Accuracy\n",
      "0.7672857142857142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Get validation predictions\n",
    "val_predictions = model(val_text_transformed.to(device).long())\n",
    "\n",
    "# Round predictions: 1 if pred>0.5, 0 otherwise\n",
    "val_predictions = np.round(val_predictions.detach().numpy())\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(val_label.numpy(), val_predictions))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(val_label.numpy(), val_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
