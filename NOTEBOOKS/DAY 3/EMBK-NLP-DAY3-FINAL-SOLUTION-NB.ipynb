{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Day 3 Solution: Use Recurrent Neural Networks (RNNs), LSTMs or Transformers for a Classification Task\n",
    "\n",
    "We continue to work with the final project dataset to see how Recurrent Neural Networks (RNNs), Long Short-term Memory Networks (LSTMs) and Transformers, perform to predict the __isPositive__ field of the dataset.\n",
    "\n",
    "* We are giving you two pieces of code to read your training and test datasets.\n",
    "* Use the notebooks from the class and implement the model, train and test with the corresponding datasets.\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "* __isPositive:__ Rating of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "! pip install -q gluonnlp mxnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "from mxnet.gluon import nn, rnn, Trainer\n",
    "from mxnet.gluon.loss import SigmoidBinaryCrossEntropyLoss\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the datasets below and fill-in the reviewText field. We will use this field as input to our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('../../DATA/NLP/EMBK-NLP-FINAL-TRAIN-CSV.csv')\n",
    "df_test = pd.read_csv('../../DATA/NLP/EMBK-NLP-FINAL-TEST-CSV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five rows in the datasets. As you can see the __log_votes__ field is numeric. That's why we will build a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>log_votes</th>\n",
       "      <th>isPositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PURCHASED FOR YOUNGSTER WHO\\nINHERITED MY \"TOO...</td>\n",
       "      <td>IDEAL FOR BEGINNER!</td>\n",
       "      <td>True</td>\n",
       "      <td>1361836800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unable to open or use</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1452643200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Waste of money!!! It wouldn't load to my system.</td>\n",
       "      <td>Dont buy it!</td>\n",
       "      <td>True</td>\n",
       "      <td>1433289600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I attempted to install this OS on two differen...</td>\n",
       "      <td>I attempted to install this OS on two differen...</td>\n",
       "      <td>True</td>\n",
       "      <td>1518912000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've spent 14 fruitless hours over the past tw...</td>\n",
       "      <td>Do NOT Download.</td>\n",
       "      <td>True</td>\n",
       "      <td>1441929600</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  PURCHASED FOR YOUNGSTER WHO\\nINHERITED MY \"TOO...   \n",
       "1                              unable to open or use   \n",
       "2   Waste of money!!! It wouldn't load to my system.   \n",
       "3  I attempted to install this OS on two differen...   \n",
       "4  I've spent 14 fruitless hours over the past tw...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                                IDEAL FOR BEGINNER!      True  1361836800   \n",
       "1                                          Two Stars      True  1452643200   \n",
       "2                                       Dont buy it!      True  1433289600   \n",
       "3  I attempted to install this OS on two differen...      True  1518912000   \n",
       "4                                   Do NOT Download.      True  1441929600   \n",
       "\n",
       "   log_votes  isPositive  \n",
       "0   0.000000         1.0  \n",
       "1   0.000000         0.0  \n",
       "2   0.000000         0.0  \n",
       "3   0.000000         0.0  \n",
       "4   1.098612         0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>log_votes</th>\n",
       "      <th>isPositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kaspersky offers the best security for your co...</td>\n",
       "      <td>State of the art protection</td>\n",
       "      <td>True</td>\n",
       "      <td>1465516800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This Value was extremely discounted which I ap...</td>\n",
       "      <td>Quickbooks</td>\n",
       "      <td>True</td>\n",
       "      <td>1393632000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some dufus probably got stock options by the t...</td>\n",
       "      <td>Sad</td>\n",
       "      <td>False</td>\n",
       "      <td>1228176000</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have reviewed the software and it is beyond ...</td>\n",
       "      <td>Excellent product</td>\n",
       "      <td>True</td>\n",
       "      <td>1402531200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Plain old simple you need Anti-Virus,I have tr...</td>\n",
       "      <td>A must have</td>\n",
       "      <td>True</td>\n",
       "      <td>1367539200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  Kaspersky offers the best security for your co...   \n",
       "1  This Value was extremely discounted which I ap...   \n",
       "2  Some dufus probably got stock options by the t...   \n",
       "3  I have reviewed the software and it is beyond ...   \n",
       "4  Plain old simple you need Anti-Virus,I have tr...   \n",
       "\n",
       "                       summary  verified        time  log_votes  isPositive  \n",
       "0  State of the art protection      True  1465516800   0.000000         1.0  \n",
       "1                   Quickbooks      True  1393632000   0.000000         1.0  \n",
       "2                          Sad     False  1228176000   2.639057         0.0  \n",
       "3            Excellent product      True  1402531200   0.000000         1.0  \n",
       "4                  A must have      True  1367539200   0.000000         1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis and Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the target distribution for our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    43692\n",
       "0.0    26308\n",
       "Name: isPositive, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"isPositive\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the number of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewText    11\n",
      "summary       14\n",
      "verified       0\n",
      "time           0\n",
      "log_votes      0\n",
      "isPositive     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewText    2\n",
      "summary       1\n",
      "verified      0\n",
      "time          0\n",
      "log_votes     0\n",
      "isPositive    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_test.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider the reviewText field. Let's fill-in the missing values for that below. We will just use the placeholder \"Missing\" here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"reviewText\"].fillna(\"Missing\", inplace=True)\n",
    "df_test[\"reviewText\"].fillna(\"Missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train-validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This separates 15% of the entire dataset into validation dataset.\n",
    "train_text, val_text, train_label, val_label = \\\n",
    "    train_test_split(df_train[\"reviewText\"].tolist(),\n",
    "                     df_train[\"isPositive\"].tolist(),\n",
    "                     test_size=0.15,\n",
    "                     random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text processing and Transformation\n",
    "We will apply the following processes here:\n",
    "* __Text cleaning:__ Simple text cleaning operations. We won't do stemming or lemmatization as our word vectors already cover different forms of words. We are using GloVe word embeddings for 6 billion words, phrases or punctuations in this example.\n",
    "* __Tokenization:__ Tokenizing all sentences\n",
    "* __Creating vocabulary:__ We will create a vocabulary of the tokens. In this vocabulary, tokens will map to unique ids, such as \"car\"->32, \"house\"->651, etc.\n",
    "* __Transforming text:__ Tokenized sentences will be mapped to unique ids. For example: [\"this\", \"is\", \"sentence\"] -> [13, 54, 412]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    }
   ],
   "source": [
    "import nltk, gluonnlp\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def cleanStr(text):\n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.lower().strip()\n",
    "    # Remove extra space and tabs\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    # Remove HTML tags/markups\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    text = cleanStr(text)\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def createVocabulary(text_list, min_freq):\n",
    "    all_tokens = []\n",
    "    for sentence in text_list:\n",
    "        all_tokens += tokenize(sentence)\n",
    "    # Calculate token frequencies\n",
    "    counter = gluonnlp.data.count_tokens(all_tokens)\n",
    "    # Create the vocabulary\n",
    "    vocab = gluonnlp.Vocab(counter,\n",
    "                           min_freq = min_freq,\n",
    "                           unknown_token = '<unk>',\n",
    "                           padding_token = None,\n",
    "                           bos_token = None,\n",
    "                           eos_token = None)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def transformText(text, vocab, max_length):\n",
    "    token_arr = np.zeros((max_length,))\n",
    "    tokens = tokenize(text)[0:max_length]\n",
    "    for idx, token in enumerate(tokens):\n",
    "        try:\n",
    "            # Use the vocabulary index of the token\n",
    "            token_arr[idx] = vocab.token_to_idx[token]\n",
    "        except:\n",
    "            token_arr[idx] = 0 # Unknown word\n",
    "    return token_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep the training time low, we only consider the first 125 words (max_length) in sentences. We also only use words that occur more than 5 times in the all sentences (min_freq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the vocabulary\n",
      "Transforming training texts\n",
      "Transforming validation texts\n"
     ]
    }
   ],
   "source": [
    "min_freq = 5\n",
    "max_length = 125\n",
    "\n",
    "print(\"Creating the vocabulary\")\n",
    "vocab = createVocabulary(train_text, min_freq)\n",
    "print(\"Transforming training texts\")\n",
    "train_text_transformed = nd.array([transformText(text, vocab, max_length) for text in train_text])\n",
    "print(\"Transforming validation texts\")\n",
    "val_text_transformed = nd.array([transformText(text, vocab, max_length) for text in val_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some unique ids for some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary index for computer: 67\n",
      "Vocabulary index for beautiful: 1923\n",
      "Vocabulary index for code: 395\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary index for computer:\", vocab['computer'])\n",
    "print(\"Vocabulary index for beautiful:\", vocab['beautiful'])\n",
    "print(\"Vocabulary index for code:\", vocab['code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Using pre-trained GloVe Word Embeddings:\n",
    "\n",
    "In this example, we will use GloVe word vectors. `'glove.6B.50d.txt'` file gives us 6 billion words/phrases vectors. Each word vector has 50 numbers in it. The following code shows how to get the word vectors and create an embedding matrix from them. We will connect our vocabulary indexes to the GloVe embedding with the `get_vecs_by_tokens()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib import text\n",
    "glove = text.embedding.create('glove',\n",
    "                              pretrained_file_name = 'glove.6B.50d.txt')\n",
    "embedding_matrix = glove.get_vecs_by_tokens(vocab.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training and validation\n",
    "\n",
    "We have processed our text data and also created our embedding matrixes from GloVe. Now, it is time to start the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set our parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the state vectors\n",
    "hidden_size = 12\n",
    "\n",
    "# General NN training parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "\n",
    "# Embedding vector and vocabulary sizes\n",
    "num_embed = 50\n",
    "vocab_size = len(vocab.token_to_idx.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to put our data into correct format before the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.data import ArrayDataset, DataLoader\n",
    "\n",
    "train_label = nd.array(train_label)\n",
    "val_label = nd.array(val_label)\n",
    "\n",
    "train_dataset = ArrayDataset(train_text_transformed, train_label)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sequential model is made of these layers:\n",
    "* Embedding layer: This is where our words/tokens are mapped to word vectors.\n",
    "* RNN layer: We will be using a simple RNN model. We won't stack RNN units in this example. It uses a sinle RNN unit with its hidden state size of 12. More details about the RNN is available [here](https://mxnet.incubator.apache.org/api/python/docs/api/gluon/rnn/index.html#mxnet.gluon.rnn.RNN). \n",
    "* Dense layer: A dense layer with a single neuron is used for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.cpu()\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add(nn.Embedding(vocab_size, num_embed), # Embedding layer\n",
    "          rnn.RNN(hidden_size),                # Recurrent layer\n",
    "          nn.Dense(1, activation=\"sigmoid\"))   # Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize this network. Then, we will need to make the embedding layer use our GloVe word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks parameters\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "# We set the embedding layer's parameters with our embedding matrix (from GloVe)\n",
    "model[0].weight.set_data(embedding_matrix.as_in_context(context))\n",
    "# We won't change/train the embedding layer\n",
    "model[0].collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the trainer and loss function below. __Binary cross-entropy loss__ is used as this is a binary classification problem.\n",
    "$$\n",
    "\\mathrm{BinaryCrossEntropyLoss} = -\\frac{1}{n}\\sum_{examples}{(y\\log(p) + (1 - y)\\log(1 - p))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting our trainer\n",
    "trainer = Trainer(model.collect_params(),\n",
    "                  'sgd',\n",
    "                  {'learning_rate': learning_rate})\n",
    "\n",
    "# We will use Sigmoid Binary Cross-entropy loss\n",
    "binary_cross_entropy_loss = SigmoidBinaryCrossEntropyLoss(from_sigmoid=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to start the training process. We will print the Binary cross-entropy loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss 0.621807 Validation_loss 0.586678 Seconds 9.037092\n",
      "Epoch 1. Train_loss 0.553893 Validation_loss 0.532601 Seconds 8.970854\n",
      "Epoch 2. Train_loss 0.519342 Validation_loss 0.512030 Seconds 9.035579\n",
      "Epoch 3. Train_loss 0.501472 Validation_loss 0.498159 Seconds 8.994958\n",
      "Epoch 4. Train_loss 0.487072 Validation_loss 0.488197 Seconds 9.114947\n",
      "Epoch 5. Train_loss 0.475217 Validation_loss 0.479580 Seconds 9.111258\n",
      "Epoch 6. Train_loss 0.465758 Validation_loss 0.473038 Seconds 9.038848\n",
      "Epoch 7. Train_loss 0.458406 Validation_loss 0.468344 Seconds 8.940323\n",
      "Epoch 8. Train_loss 0.452745 Validation_loss 0.464891 Seconds 8.859067\n",
      "Epoch 9. Train_loss 0.448174 Validation_loss 0.462168 Seconds 9.030634\n",
      "Epoch 10. Train_loss 0.444210 Validation_loss 0.459715 Seconds 8.995466\n",
      "Epoch 11. Train_loss 0.440575 Validation_loss 0.456971 Seconds 9.041262\n",
      "Epoch 12. Train_loss 0.437049 Validation_loss 0.454920 Seconds 9.100632\n",
      "Epoch 13. Train_loss 0.433966 Validation_loss 0.452838 Seconds 8.930927\n",
      "Epoch 14. Train_loss 0.431217 Validation_loss 0.451023 Seconds 8.886883\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    # Training loop, train the network\n",
    "    for idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.as_in_context(context)\n",
    "        target = target.as_in_context(context)\n",
    "        \n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            \n",
    "            L = binary_cross_entropy_loss(output, target)\n",
    "            training_loss += nd.sum(L).asscalar()\n",
    "            L.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    val_predictions = model(val_text_transformed.as_in_context(context))\n",
    "    val_loss = nd.sum(binary_cross_entropy_loss(val_predictions, val_label)).asscalar()\n",
    "    \n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(train_label)\n",
    "    val_loss = val_loss / len(val_label)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Epoch %s. Train_loss %f Validation_loss %f Seconds %f\" % \\\n",
    "          (epoch, training_loss, val_loss, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained it for 15 epochs. As you can see, the validation loss goes down with each epoch, feel free to increase the number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test performance\n",
    "Let's see how the model performs for the test data. We will use some of the sklearn's metric functions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.72      2807\n",
      "           1       0.86      0.82      0.84      5193\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      8000\n",
      "   macro avg       0.77      0.78      0.78      8000\n",
      "weighted avg       0.80      0.79      0.79      8000\n",
      "\n",
      "Accuracy\n",
      "0.793125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "test_text = df_test[\"reviewText\"].tolist()\n",
    "test_label = df_test[\"isPositive\"].tolist()\n",
    "\n",
    "# Transform test text\n",
    "test_text_transformed = nd.array([transformText(text, vocab, max_length) for text in test_text])\n",
    "# Get test predictions\n",
    "test_predictions = model(test_text_transformed.as_in_context(context))\n",
    "\n",
    "# Map the predictions to 0 and 1\n",
    "mapped_predictions = []\n",
    "for pred in test_predictions.asnumpy():\n",
    "    if pred[0]>0.5:\n",
    "        mapped_predictions.append(1)\n",
    "    else:\n",
    "        mapped_predictions.append(0)\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(mapped_predictions, test_label))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(mapped_predictions, test_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
