{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs) for the Regression Problem\n",
    "\n",
    "In this notebook, we build a Recurrent Neural Networks (RNNs) using GloVe word embeddings, to predict the __log_votes__ field of our review dataset.\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __rating:__ Rating of the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "\n",
    "__Important note:__ One big distinction betweeen the regular neural networks and RNNs is that RNNs work with sequential data. In our case, RNNs will help us with the text field. If we also want to consider other fields such as time, rating, verified, etc. , we need to use the regular neural networks and connect it to the RNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "from mxnet.gluon import nn, rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset below and fill-in the reviewText field. We will use this field as input to our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../DATA/NLP/EMBK-NLP-REVIEW-DATA-CSV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five rows in the dataset. As you can see the __log_votes__ field is numeric. That's why we will build a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>log_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuck with this at work, slow and we still got...</td>\n",
       "      <td>Use SEP or Mcafee</td>\n",
       "      <td>False</td>\n",
       "      <td>1464739200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I use parallels every day with both my persona...</td>\n",
       "      <td>Use it daily</td>\n",
       "      <td>False</td>\n",
       "      <td>1332892800</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barbara Robbins\\n\\nI've used TurboTax to do ou...</td>\n",
       "      <td>Helpful Product</td>\n",
       "      <td>True</td>\n",
       "      <td>1398816000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have been using this software security for y...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1430784000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you want your computer hijacked and slowed ...</td>\n",
       "      <td>... hijacked and slowed to a crawl Windows 10 ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1508025600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  Stuck with this at work, slow and we still got...   \n",
       "1  I use parallels every day with both my persona...   \n",
       "2  Barbara Robbins\\n\\nI've used TurboTax to do ou...   \n",
       "3  I have been using this software security for y...   \n",
       "4  If you want your computer hijacked and slowed ...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                                  Use SEP or Mcafee     False  1464739200   \n",
       "1                                       Use it daily     False  1332892800   \n",
       "2                                    Helpful Product      True  1398816000   \n",
       "3                                         Five Stars      True  1430784000   \n",
       "4  ... hijacked and slowed to a crawl Windows 10 ...     False  1508025600   \n",
       "\n",
       "   rating  log_votes  \n",
       "0     1.0        0.0  \n",
       "1     5.0        0.0  \n",
       "2     4.0        0.0  \n",
       "3     5.0        0.0  \n",
       "4     1.0        0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis and Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the range and distribution of log_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"log_votes\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.799753318287247"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"log_votes\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"log_votes\"].plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewText    6\n",
      "summary       7\n",
      "verified      0\n",
      "time          0\n",
      "rating        0\n",
      "log_votes     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider the reviewText field. Let's fill-in the missing values for that below. We will just use the placeholder \"Missing\" here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviewText\"].fillna(\"Missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text processing-cleaning\n",
    "Next, we will clean the text. We will remove leading/train white space, extra space and html tags. Recurrent neural networks usually __DON'T__ need text processing work further than simple text cleaning. Stemming and lemmatization can introduce some errors that will cause our model to skip those words completely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some string preprocessing\n",
    "def clean_str(text):\n",
    "    text = text.lower().strip() # Remove leading/trailing whitespace\n",
    "    text = re.sub('\\s+', ' ', text) # Remove extra space and tabs\n",
    "    text = re.compile('<.*?>').sub('', text) # Remove HTML tags/markups:\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to process all of the words in the reviews, count the number of occurences of each word, and then index the words in descending order with respect to how many times this occur. This is a necessary input to help us encode the words in the reviews so that they can be understood by a machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a dictionary of the words and their counts\n",
    "\n",
    "word_counter = Counter()\n",
    "def create_count(sentiments):\n",
    "    for line in sentiments:\n",
    "        for word in (clean_str(line)).split():\n",
    "            if word not in word_counter.keys():               \n",
    "                word_counter[word] = 1\n",
    "            else:\n",
    "                word_counter[word] += 1\n",
    "\n",
    "#This assigns a unique a number for each word (sorted by descending order \n",
    "#based on the frequency of occurrence)and returns a word_dict\n",
    "\n",
    "def create_word_index():\n",
    "    idx = 1\n",
    "    word_dict = {}\n",
    "    for word in word_counter.most_common():\n",
    "        word_dict[word[0]] = idx\n",
    "        idx+=1\n",
    "    return word_dict\n",
    "    \n",
    "#Here we combine all of the reviews into one dataset and create a word\n",
    "#dictionary using this entire dataset\n",
    "create_count(df[\"reviewText\"].tolist())\n",
    "word_dict = create_word_index()\n",
    "\n",
    "#This creates a reverse index from a number to the word \n",
    "idx2word = {v: k for k, v in word_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a set of helper functions that (1) encode words into a sequence of numbers, (2) decode a sequence of numbers back into words, and (3) truncate and pad the input data to ensure they are of equal length and thereby enable easier processing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This helper function creates a encoded sentences by assigning the unique \n",
    "#id from word_dict to the words in the input text\n",
    "def encoded_sentences(input_file,word_dict):\n",
    "    output_string = []\n",
    "    for line in input_file:\n",
    "        output_line = []\n",
    "        for word in (clean_str(line)).split():\n",
    "            if word in word_dict:\n",
    "                output_line.append(word_dict[word])\n",
    "        output_string.append(output_line)\n",
    "    return output_string\n",
    "\n",
    "#This helper function decodes encoded sentences\n",
    "def decode_sentences(input_file,word_dict):\n",
    "    output_string = []\n",
    "    for line in input_file:\n",
    "        output_line = ''\n",
    "        for idx in line:\n",
    "            output_line += idx2word[idx] + ' '\n",
    "        output_string.append(output_line)\n",
    "    return output_string\n",
    "\n",
    "#This helper function pads the sequences to maxlen.\n",
    "#If the sentence is greater than maxlen, it truncates the sentence.\n",
    "#If the sentence is less than 50, it pads with value 0.\n",
    "def pad_sequences(sentences,maxlen=50,value=0):\n",
    "    padded_sentences = []\n",
    "    for sen in sentences:\n",
    "        new_sentence = []\n",
    "        if(len(sen) > maxlen):\n",
    "            new_sentence = sen[:maxlen]\n",
    "            padded_sentences.append(new_sentence)\n",
    "        else:\n",
    "            num_padding = maxlen - len(sen)\n",
    "            new_sentence = np.append(sen,[value] * num_padding)\n",
    "            padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to encode all of the reviewText using the word dictionary created. In addition, we are going to cap the size of the tracked vocabulary size - meaning any word that is outside of the tracked range will be encoded with the last position. This is performance versus accuracy consideration - a larger tracked vocabulary will lead to more accurary but will have performance considerations because it requires a longer training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's encode sentences\n",
    "encoded_texts = encoded_sentences(df[\"reviewText\"].tolist(), word_dict)\n",
    "\n",
    "#Here we set the total num of words to be tracked\n",
    "vocab_size = 5000 \n",
    "\n",
    "#Any word outside of the tracked range will be encoded with last position.\n",
    "t_data = [np.array([i if i<(vocab_size-1) else (vocab_size-1) for i in s]) for s in encoded_texts]\n",
    "all_labels = df[\"log_votes\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1095,   15,   10,   42,  751,  449,    4,  102,  101,  117, 1147,\n",
       "         15,    5, 4999, 4999])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the first sentence\n",
    "# We have 4999 for words outside range of 5000 words\n",
    "\n",
    "t_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using pre-trained GloVe Word Embeddings:\n",
    "\n",
    "In this example, we will use GloVe word vectors. The following code shows how to get the word vectors and create an embedding dictionary with them. The dictionary maps the words to their word vectors. The file is downloaded from here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-12 03:44:42--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2020-02-12 03:44:42--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2020-02-12 03:44:42--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip.1’\n",
      "\n",
      "glove.6B.zip.1      100%[===================>] 822.24M  1.81MB/s    in 6m 29s  \n",
      "\n",
      "2020-02-12 03:51:11 (2.11 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the zip file - WARNING: THIS TAKES A WHILE!\n",
    "! wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "# Unziping\n",
    "! unzip glove.6B.zip\n",
    "\n",
    "# Deleting the zip file\n",
    "! rm glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we first create a mapper for the word vectors (word->vector) with the __load_glove_index()__ function. Later, __create_emb()__ creates an embedding matrix. Each row corresponds to a word. For our vocabulary size of 5000 and 300 word vector dimension, this gives a matrix of 5000 rows and  300 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We downloaded the 300 dimension word vectors\n",
    "num_embed = 300 \n",
    "\n",
    "def load_glove_index(loc):\n",
    "    f = open(loc, encoding=\"utf8\")\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "def create_emb():\n",
    "    embedding_matrix = np.zeros((vocab_size, num_embed))\n",
    "    for word, i in word_dict.items():\n",
    "        if i >= vocab_size:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    embedding_matrix = nd.array(embedding_matrix)\n",
    "    return embedding_matrix\n",
    "\n",
    "embeddings_index = load_glove_index('glove.6B.300d.txt')\n",
    "embedding_matrix = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we prepare the review texts to be fed into the deep learning model by (1) Reserving 15% of the dataset as a validation dataset, (2) padding and truncating the data to the length of 50 words, and (3) converting the encoded text into into MXNet's NDArray format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This separates 15% of the entire dataset into test dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(t_data, all_labels, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This set the max word length of each review text\n",
    "seq_len = 50\n",
    "\n",
    "#Below we pad the reviews and convert them to MXNet's NDArray format\n",
    "X_train = nd.array(pad_sequences(X_train, maxlen=seq_len, value=0))\n",
    "y_train = nd.array(y_train)\n",
    "X_test = nd.array(pad_sequences(X_test, maxlen=seq_len, value=0))\n",
    "y_test = nd.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set our parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 64\n",
    "learning_rate = .001\n",
    "epochs = 10\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arraydataset = mx.gluon.data.ArrayDataset(X_train, y_train)\n",
    "train_loader = mx.gluon.data.DataLoader(train_arraydataset, batch_size=batch_size, shuffle=False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using an RNN model with 64 hidden units. Let's use the Sequential mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.cpu()\n",
    "\n",
    "model = mx.gluon.nn.Sequential()\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add(mx.gluon.nn.Embedding(vocab_size, num_embed),        # Embedding layer\n",
    "          mx.gluon.rnn.RNN(num_hidden, layout = 'NTC'),        # Recurrent layer\n",
    "          mx.gluon.nn.Dense(1, activation='relu')              # Output layer\n",
    "         )\n",
    "\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "model[0].weight.set_data(embedding_matrix.as_in_context(context))\n",
    "model[0].collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we execute the training loop, we need to define a function that will calculate the accurary metrics for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the training process below. We will print Mean Squared Error after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mse(data_iterator, net, ctx=mx.cpu()):\n",
    "    metric = mx.metric.MSE()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data1 = batch.data[0].as_in_context(ctx)\n",
    "        data2 = batch.data[1].as_in_context(ctx)\n",
    "        data = [data1, data2]\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        metric.update([label], [output])\n",
    "    return metric.get()[1]\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net, ctx=mx.cpu()):\n",
    "    metric = mx.metric.MSE()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data1 = batch.data[0].as_in_context(ctx)\n",
    "        data2 = batch.data[1].as_in_context(ctx)\n",
    "        data = [data1, data2]\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        metric.update([label], [output])\n",
    "    return metric.get()[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss (mse) 0.36823840090871496 Validation_loss (mse) 0.3981466027462121\n",
      "Epoch 1. Train_loss (mse) 0.36229212773228714 Validation_loss (mse) 0.396168974905303\n",
      "Epoch 2. Train_loss (mse) 0.3576843620143472 Validation_loss (mse) 0.3949965672348485\n",
      "Epoch 3. Train_loss (mse) 0.3538548159267813 Validation_loss (mse) 0.3942821081912879\n",
      "Epoch 4. Train_loss (mse) 0.35053092534210595 Validation_loss (mse) 0.3937601503314394\n",
      "Epoch 5. Train_loss (mse) 0.34753742843069496 Validation_loss (mse) 0.3934677142518939\n",
      "Epoch 6. Train_loss (mse) 0.34479565669317297 Validation_loss (mse) 0.39322253787878786\n",
      "Epoch 7. Train_loss (mse) 0.34222613111791766 Validation_loss (mse) 0.39326376065340907\n",
      "Epoch 8. Train_loss (mse) 0.33981642869003315 Validation_loss (mse) 0.39332572798295456\n",
      "Epoch 9. Train_loss (mse) 0.337520366230113 Validation_loss (mse) 0.39346851325757576\n"
     ]
    }
   ],
   "source": [
    "# Setting our trainer\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': learning_rate})\n",
    "\n",
    "# We will use L2 loss (regression)\n",
    "l2_loss = gluon.loss.L2Loss()    \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    training_loss = 0\n",
    "    train_predictions = nd.array([])\n",
    "    # Training loop, train the network\n",
    "    for idx,(data,target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.as_in_context(context)\n",
    "        target = target.as_in_context(context)\n",
    "        \n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            \n",
    "            L = l2_loss(output, target)\n",
    "            training_loss += nd.sum(L).asscalar()\n",
    "            #print(idx, training_loss)\n",
    "            if len(train_predictions)>0:\n",
    "                train_predictions = nd.concat(train_predictions, output, dim=0)\n",
    "            else:\n",
    "                train_predictions = output\n",
    "            \n",
    "            L.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "    \n",
    "    val_predictions = model(X_test)\n",
    "    val_loss = nd.sum(l2_loss(val_predictions, y_test)).asscalar()\n",
    "    \n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(X_train)\n",
    "    val_loss = val_loss / len(val_predictions)\n",
    "      \n",
    "    print(\"Epoch %s. Train_loss (mse) %s Validation_loss (mse) %s\" % (epoch, training_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘glove.6B.100d.txt’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Deleting notebook artifacts\n",
    "! rm glove.6B.300d.txt\n",
    "! rm glove.6B.200d.txt\n",
    "! rm glove.6B.100d.txt\n",
    "! rm glove.6B.50d.txt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
