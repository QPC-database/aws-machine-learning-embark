{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-Based Models for a Regression Problem and Hyperparameter Tuning\n",
    "\n",
    "We continue to work with our review dataset to see how Tree-based regressors (Decision Tree, Random Forest), along with efficient optimization techniques (GridSearch, RandomizedSearch), perform to predict the __log_votes__ field of the dataset.\n",
    "\n",
    "* Find more details on the DecisionTreeRegressor here: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "\n",
    "* Find more details on the RandomForestRegressor here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "* Find more details on the GridSearchCV here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "* Find more details on the RandomizedSearchCV here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __rating:__ Rating of the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading the dataset\n",
    "\n",
    "We will use the __pandas__ library to read our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is: (55000, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_csv('../../DATA/NLP/EMBK-NLP-REVIEW-DATA-CSV.csv')\n",
    "\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five rows in the dataset. As you can see the __log_votes__ field is numeric. That's why we will build a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>log_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuck with this at work, slow and we still got...</td>\n",
       "      <td>Use SEP or Mcafee</td>\n",
       "      <td>False</td>\n",
       "      <td>1464739200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I use parallels every day with both my persona...</td>\n",
       "      <td>Use it daily</td>\n",
       "      <td>False</td>\n",
       "      <td>1332892800</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barbara Robbins\\n\\nI've used TurboTax to do ou...</td>\n",
       "      <td>Helpful Product</td>\n",
       "      <td>True</td>\n",
       "      <td>1398816000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have been using this software security for y...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1430784000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you want your computer hijacked and slowed ...</td>\n",
       "      <td>... hijacked and slowed to a crawl Windows 10 ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1508025600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  Stuck with this at work, slow and we still got...   \n",
       "1  I use parallels every day with both my persona...   \n",
       "2  Barbara Robbins\\n\\nI've used TurboTax to do ou...   \n",
       "3  I have been using this software security for y...   \n",
       "4  If you want your computer hijacked and slowed ...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                                  Use SEP or Mcafee     False  1464739200   \n",
       "1                                       Use it daily     False  1332892800   \n",
       "2                                    Helpful Product      True  1398816000   \n",
       "3                                         Five Stars      True  1430784000   \n",
       "4  ... hijacked and slowed to a crawl Windows 10 ...     False  1508025600   \n",
       "\n",
       "   rating  log_votes  \n",
       "0     1.0        0.0  \n",
       "1     5.0        0.0  \n",
       "2     4.0        0.0  \n",
       "3     5.0        0.0  \n",
       "4     1.0        0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis and Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the range and distribution of log_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"log_votes\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.799753318287247"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"log_votes\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"log_votes\"].plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewText    6\n",
      "summary       7\n",
      "verified      0\n",
      "time          0\n",
      "rating        0\n",
      "log_votes     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill-in the missing values for reviewText below. We will just use the placeholder \"Missing\" here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviewText\"].fillna(\"Missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stop Word Removal and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the stop word removal and text cleaning processes below. NLTK library provides a list of common stop words. We will use the list, but remove some of the words from that list (because those words are actually useful to understand the sentiment in the sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Let's get a list of stop words from the NLTK library\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# These words are important for our problem. We don't want to remove them.\n",
    "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "             'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn',\n",
    "            \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "# New stop word list\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "def process_text(texts): \n",
    "    final_text_list=[]\n",
    "    for sent in texts:\n",
    "        filtered_sentence=[]\n",
    "        \n",
    "        sent = sent.lower() # Lowercase \n",
    "        sent = sent.strip() # Remove leading/trailing whitespace\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
    "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
    "        \n",
    "        for w in word_tokenize(sent):\n",
    "            # We are applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words):  \n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence) #final string of cleaned words\n",
    " \n",
    "        final_text_list.append(final_string)\n",
    "    \n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing the reviewText field\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-processing the reviewText field\")\n",
    "df[\"reviewText\"] = process_text(df[\"reviewText\"].tolist()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scaling numerical fields:\n",
    "\n",
    "We will apply min-max scaling to our rating field so that they will be between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rating\"] = (df[\"rating\"] - df[\"rating\"].min())/ (df[\"rating\"].max()-df[\"rating\"].min())\n",
    "df[\"time\"] = (df[\"time\"] - df[\"time\"].min())/ (df[\"time\"].max()-df[\"time\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Splitting the training dataset into training and validation\n",
    "\n",
    "Sklearn library has a useful function to split datasets. We will use the __train_test_split()__ function. In the example below, we get 90% of the data for training and 10% is left for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[[\"reviewText\", \"rating\", \"time\"]], # Using these three fields\n",
    "                                                  df[\"log_votes\"].tolist(),  # Target field\n",
    "                                                  test_size=0.10,  # 10% test, 90% tranining\n",
    "                                                  shuffle=True # Shuffle the whole dataset\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Computing Bag of Words Features\n",
    "\n",
    "We are using binary features here. TF and TF-IDF are other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Initialize the binary count vectorizer\n",
    "tfidf_vectorizer = CountVectorizer(binary=True,\n",
    "                                   max_features=50    # Limit the vocabulary size\n",
    "                                  )\n",
    "\n",
    "X_train_text_vectors = tfidf_vectorizer.fit_transform(X_train[\"reviewText\"].tolist()) # Fit and transform\n",
    "X_val_text_vectors = tfidf_vectorizer.transform(X_val[\"reviewText\"].tolist())       # Only transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print our vocabulary below. The number next to the word is its index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work': 47,\n",
       " 'great': 13,\n",
       " 'price': 25,\n",
       " 'get': 11,\n",
       " 'softwar': 33,\n",
       " 'look': 17,\n",
       " 'new': 22,\n",
       " 'run': 32,\n",
       " 'time': 36,\n",
       " 'find': 10,\n",
       " 'use': 39,\n",
       " 'good': 12,\n",
       " 'buy': 3,\n",
       " 'not': 23,\n",
       " 'realli': 30,\n",
       " 'much': 20,\n",
       " 'year': 49,\n",
       " 'instal': 15,\n",
       " 'would': 48,\n",
       " 'also': 0,\n",
       " 'comput': 4,\n",
       " 'problem': 26,\n",
       " 'version': 42,\n",
       " 'need': 21,\n",
       " 'tri': 37,\n",
       " 'download': 6,\n",
       " 'purchas': 29,\n",
       " 'product': 27,\n",
       " 'upgrad': 38,\n",
       " 'support': 35,\n",
       " 'help': 14,\n",
       " 'make': 18,\n",
       " 'back': 1,\n",
       " 'program': 28,\n",
       " 'even': 8,\n",
       " 'well': 45,\n",
       " 'still': 34,\n",
       " 'easi': 7,\n",
       " 'like': 16,\n",
       " 'user': 40,\n",
       " 'way': 44,\n",
       " 'mani': 19,\n",
       " 'recommend': 31,\n",
       " 'one': 24,\n",
       " 'window': 46,\n",
       " 'file': 9,\n",
       " 'want': 43,\n",
       " 've': 41,\n",
       " 'could': 5,\n",
       " 'better': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge our features to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train_features = np.column_stack((X_train_text_vectors.toarray(), \n",
    "                                    X_train[\"rating\"].values, \n",
    "                                    X_train[\"time\"].values)\n",
    "                                  )\n",
    "X_val_features = np.column_stack((X_val_text_vectors.toarray(), \n",
    "                                  X_val[\"rating\"].values,\n",
    "                                  X_val[\"time\"].values)\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Fitting tree-based regressors and checking the validation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1  DecisionTreeRegressor\n",
    "Let's first fit a __DecisionTreeRegressor__ from Sklearn library, and check the performance on the validation dataset.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressor on Validation: R_square_score: 0.305673, Mean_squared_error: 0.646136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "dtRegressor = DecisionTreeRegressor(max_depth = 10, min_samples_leaf = 15)\n",
    "dtRegressor.fit(X_train_features, y_train)\n",
    "dtRegressor_val_predictions = dtRegressor.predict(X_val_features)\n",
    "print(\"DecisionTreeRegressor on Validation: R_square_score: %f, Mean_squared_error: %f\" % (r2_score(y_val, dtRegressor_val_predictions), mean_squared_error(y_val, dtRegressor_val_predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2  RandomForestRegressor\n",
    "Let's now fit a __RandomForestRegressor__ from Sklearn library, and check the performance on the validation dataset.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor on Validation: R_square_score: 0.387090, Mean_squared_error: 0.570370\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "rfRegressor = RandomForestRegressor(n_estimators = 200, max_depth = 10, min_samples_leaf = 15)\n",
    "rfRegressor.fit(X_train_features, y_train)\n",
    "rfRegressor_val_predictions =rfRegressor.predict(X_val_features)\n",
    "print(\"RandomForestRegressor on Validation: R_square_score: %f, Mean_squared_error: %f\" % (r2_score(y_val, rfRegressor_val_predictions), mean_squared_error(y_val, rfRegressor_val_predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Hyperparameter Tuning\n",
    "\n",
    "Let's try different parameter values and see how the __RandomForestRegressor__ model performs under some combinations of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 GridSearchCV\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "  \n",
    "rf = RandomForestRegressor()\n",
    "parameters = {'n_estimators': [200, 300, 400], 'max_depth': [10, 20], 'min_samples_leaf': [15, 25]}\n",
    "                      \n",
    "# NOTE: GridSearchCV uses by default the score function of the estimator to evaluate (r2_score for regression; accuracy_score for classification). If desired, other scoring functions can be specified via the 'scoring' parameter. \n",
    "# NOTE: You can experiment with different cv numbers, default = 5\n",
    "regressor_grid = GridSearchCV(rf, parameters, cv=5, verbose=1, n_jobs=-1)\n",
    "regressor_grid.fit(X_train_features, y_train)\n",
    "\n",
    "print(\"Best parameters: \", regressor_grid.best_params_)\n",
    "print(\"Best score: \", regressor_grid.best_score_)\n",
    "    \n",
    "regressor_grid_val_predictions = regressor_grid.best_estimator_.predict(X_val_features)\n",
    "\n",
    "print(\"RandomForestRegressor with GridSearchCV on Validation: R_square_score: %f, Mean_squared_error: %f\" % (r2_score(y_val, regressor_grid_val_predictions), mean_squared_error(y_val, regressor_grid_val_predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 RandomizedSearchCV\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "  \n",
    "rf = RandomForestRegressor()\n",
    "parameters = {'n_estimators': [200, 300, 400], 'max_depth': [10, 20], 'min_samples_leaf': [15, 25]}\n",
    "                      \n",
    "# NOTE: RandomizedSearchCV uses by default the score function of the estimator to evaluate (r2_score for regression; accuracy_score for classification). If desired, other scoring functions can be specified via the 'scoring' parameter. \n",
    "# NOTE: You can experiment with different cv numbers, default = 5\n",
    "# NOTE: You can also experiment with different n_iter (number of parameter settings that are sampled by the RandomizedSearch), default = 10\n",
    "regressor_rand = RandomizedSearchCV(rf, parameters, cv=5, verbose=1, n_jobs=-1)\n",
    "regressor_rand.fit(X_train_features, y_train)\n",
    "\n",
    "print(\"Best parameters: \", regressor_rand.best_params_)\n",
    "print(\"Best score: \", regressor_rand.best_score_)\n",
    "    \n",
    "regressor_rand_val_predictions = regressor_rand.best_estimator_.predict(X_val_features)\n",
    "\n",
    "print(\"RandomForestRegressor with RandomizedSearchCV on Validation: R_square_score: %f, Mean_squared_error: %f\" % (r2_score(y_val, regressor_rand_val_predictions), mean_squared_error(y_val, regressor_rand_val_predictions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Ideas for improvement\n",
    "\n",
    "**Preprocessing**: We can usually improve performance with some additional work. You can try the following:\n",
    "* Change the feature extractor to TF, TF-IDF. Also experiment with different vocabulary size.\n",
    "* Add the other text field __summary__ to the model and get bag of words features of it.\n",
    "* Come up with some other features such as having certain punctuations, all-capitalized words or some words that might be useful in this problem.\n",
    "\n",
    "**Hyperparameter Tuning**: Alsways a good idea to try other parameter ranges and/or combinations of parameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
