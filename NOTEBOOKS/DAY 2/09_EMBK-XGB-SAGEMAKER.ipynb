{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model - SageMaker\n",
    "\n",
    "In this notebook, we build, train a [__XGBoost Classifier__](https://github.com/dmlc/xgboost) to predict the __Outcome Type__ field of our Austin Animal dataset.\n",
    "\n",
    "\n",
    "1. <a href=\"#1\">Read the dataset</a>\n",
    "2. <a href=\"#2\">Exploratory Data Analysis</a>\n",
    "3. <a href=\"#3\">Select features to build the model</a>\n",
    "4. <a href=\"#4\">Training and test datasets</a>\n",
    "5. <a href=\"#5\">Data processing with ColumnTransformer</a>\n",
    "6. <a href=\"#6\">Train a classifier using SageMaker</a>\n",
    "7. <a href=\"#7\">Deploy and test the classifier using SageMaker</a>\n",
    "\n",
    "__Notes on [AWS SageMaker](https://docs.aws.amazon.com/sagemaker/index.html):__\n",
    "\n",
    "* Fully managed machine learning service, to quickly and easily get you started on building and training machine learning models - we have seen that already! Integrated Jupyter notebook instances, with easy access to data sources for exploration and analysis, abstract away many of the messy infrastructural details needed for hands-on ML - you don't have to manage servers, install libraries/dependencies, etc.!\n",
    "\n",
    "\n",
    "* Apart from building custom machine learning models in SageMaker notebooks, like we did so far, SageMaker also provides a few [built-in common machine learning algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) (check \"SageMaker Examples\" from your SageMaker instance top menu for a complete updated list) that are optimized to run efficiently against extremely large data in a distributed environment. The trained model can then be directly deployed into a production-ready hosted environment for easy access at inference. \n",
    "\n",
    "__Austin Animal Center Dataset__:\n",
    "\n",
    "In this exercise, we are working with pet adoption data from __Austin Animal Center__. We have two datasets that cover intake and outcome of animals. Intake data is available from [here](https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Intakes/wter-evkm) and outcome is from [here](https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Outcomes/9t4d-g238). \n",
    "\n",
    "In order to work with a single table, we joined the intake and outcome tables using the \"Animal ID\" column and created a single __Austin_Animal_dataset.csv__ file. We also didn't consider animals with multiple entries to the facility to keep our dataset simple. If you want to see the original datasets and the merged data with multiple entries, they are available under `DATA/austin-animal` folder: Austin_Animal_Center_Intakes.csv, Austin_Animal_Center_Outcomes.csv and Austin_Animal_Center_Intakes_Outcomes.csv.\n",
    "\n",
    "__Dataset schema:__ \n",
    "- __Pet ID__ - Unique ID of pet\n",
    "- __Outcome Type__ - State of pet at the time of recording the outcome (0 = not placed, 1 = placed). This is the field to predict.\n",
    "- __Sex upon Outcome__ - Sex of pet at outcome\n",
    "- __Name__ - Name of pet \n",
    "- __Found Location__ - Found location of pet before entered the center\n",
    "- __Intake Type__ - Circumstances bringing the pet to the center\n",
    "- __Intake Condition__ - Health condition of pet when entered the center\n",
    "- __Pet Type__ - Type of pet\n",
    "- __Sex upon Intake__ - Sex of pet when entered the center\n",
    "- __Breed__ - Breed of pet \n",
    "- __Color__ - Color of pet \n",
    "- __Age upon Intake Days__ - Age of pet when entered the center (days)\n",
    "- __Age upon Outcome Days__ - Age of pet at outcome (days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Upgrade dependencies\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Read the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's read the dataset into a dataframe, using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is: (95485, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "  \n",
    "df = pd.read_csv('../../DATA/austin-animal/Austin_Animal_dataset.csv')\n",
    "\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Exploratory Data Analysis</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will look at number of rows, columns and some simple statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pet ID</th>\n",
       "      <th>Outcome Type</th>\n",
       "      <th>Sex upon Outcome</th>\n",
       "      <th>Name</th>\n",
       "      <th>Found Location</th>\n",
       "      <th>Intake Type</th>\n",
       "      <th>Intake Condition</th>\n",
       "      <th>Pet Type</th>\n",
       "      <th>Sex upon Intake</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "      <th>Age upon Intake Days</th>\n",
       "      <th>Age upon Outcome Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A794011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>Chunk</td>\n",
       "      <td>Austin (TX)</td>\n",
       "      <td>Owner Surrender</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Brown Tabby/White</td>\n",
       "      <td>730</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A776359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>Gizmo</td>\n",
       "      <td>7201 Levander Loop in Austin (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>Chihuahua Shorthair Mix</td>\n",
       "      <td>White/Brown</td>\n",
       "      <td>365</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A674754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12034 Research in Austin (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Orange Tabby</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A689724</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>*Donatello</td>\n",
       "      <td>2300 Waterway Bnd in Austin (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Black</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A680969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>*Zeus</td>\n",
       "      <td>4701 Staggerbrush Rd in Austin (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Nursing</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>White/Orange Tabby</td>\n",
       "      <td>7</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pet ID  Outcome Type Sex upon Outcome        Name  \\\n",
       "0  A794011           1.0    Neutered Male       Chunk   \n",
       "1  A776359           1.0    Neutered Male       Gizmo   \n",
       "2  A674754           0.0      Intact Male         NaN   \n",
       "3  A689724           1.0    Neutered Male  *Donatello   \n",
       "4  A680969           1.0    Neutered Male       *Zeus   \n",
       "\n",
       "                        Found Location      Intake Type Intake Condition  \\\n",
       "0                          Austin (TX)  Owner Surrender           Normal   \n",
       "1    7201 Levander Loop in Austin (TX)            Stray           Normal   \n",
       "2        12034 Research in Austin (TX)            Stray          Nursing   \n",
       "3     2300 Waterway Bnd in Austin (TX)            Stray           Normal   \n",
       "4  4701 Staggerbrush Rd in Austin (TX)            Stray          Nursing   \n",
       "\n",
       "  Pet Type Sex upon Intake                    Breed               Color  \\\n",
       "0      Cat   Neutered Male   Domestic Shorthair Mix   Brown Tabby/White   \n",
       "1      Dog     Intact Male  Chihuahua Shorthair Mix         White/Brown   \n",
       "2      Cat     Intact Male   Domestic Shorthair Mix        Orange Tabby   \n",
       "3      Cat     Intact Male   Domestic Shorthair Mix               Black   \n",
       "4      Cat     Intact Male   Domestic Shorthair Mix  White/Orange Tabby   \n",
       "\n",
       "   Age upon Intake Days  Age upon Outcome Days  \n",
       "0                   730                    730  \n",
       "1                   365                    365  \n",
       "2                     6                      6  \n",
       "3                    60                     60  \n",
       "4                     7                     60  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five rows\n",
    "# NaN means missing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95485 entries, 0 to 95484\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Pet ID                 95485 non-null  object \n",
      " 1   Outcome Type           95485 non-null  float64\n",
      " 2   Sex upon Outcome       95484 non-null  object \n",
      " 3   Name                   59138 non-null  object \n",
      " 4   Found Location         95485 non-null  object \n",
      " 5   Intake Type            95485 non-null  object \n",
      " 6   Intake Condition       95485 non-null  object \n",
      " 7   Pet Type               95485 non-null  object \n",
      " 8   Sex upon Intake        95484 non-null  object \n",
      " 9   Breed                  95485 non-null  object \n",
      " 10  Color                  95485 non-null  object \n",
      " 11  Age upon Intake Days   95485 non-null  int64  \n",
      " 12  Age upon Outcome Days  95485 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(10)\n",
      "memory usage: 9.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Let's see the data types and non-null values for each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outcome Type</th>\n",
       "      <th>Age upon Intake Days</th>\n",
       "      <th>Age upon Outcome Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>95485.000000</td>\n",
       "      <td>95485.000000</td>\n",
       "      <td>95485.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.564005</td>\n",
       "      <td>703.436959</td>\n",
       "      <td>717.757313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.495889</td>\n",
       "      <td>1052.252197</td>\n",
       "      <td>1055.023160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9125.000000</td>\n",
       "      <td>9125.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Outcome Type  Age upon Intake Days  Age upon Outcome Days\n",
       "count  95485.000000          95485.000000           95485.000000\n",
       "mean       0.564005            703.436959             717.757313\n",
       "std        0.495889           1052.252197            1055.023160\n",
       "min        0.000000              0.000000               0.000000\n",
       "25%        0.000000             30.000000              60.000000\n",
       "50%        1.000000            365.000000             365.000000\n",
       "75%        1.000000            730.000000             730.000000\n",
       "max        1.000000           9125.000000            9125.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This prints basic statistics for numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate model features and model target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pet ID', 'Outcome Type', 'Sex upon Outcome', 'Name', 'Found Location',\n",
      "       'Intake Type', 'Intake Condition', 'Pet Type', 'Sex upon Intake',\n",
      "       'Breed', 'Color', 'Age upon Intake Days', 'Age upon Outcome Days'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model features:  Index(['Pet ID', 'Sex upon Outcome', 'Name', 'Found Location', 'Intake Type',\n",
      "       'Intake Condition', 'Pet Type', 'Sex upon Intake', 'Breed', 'Color',\n",
      "       'Age upon Intake Days', 'Age upon Outcome Days'],\n",
      "      dtype='object')\n",
      "Model target:  Outcome Type\n"
     ]
    }
   ],
   "source": [
    "model_features = df.columns.drop('Outcome Type')\n",
    "model_target = 'Outcome Type'\n",
    "\n",
    "print('Model features: ', model_features)\n",
    "print('Model target: ', model_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the features set further, figuring out first what features are numerical or categorical. Beware that some integer-valued features could actually be categorical features, and some categorical features could be text features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: Index(['Age upon Intake Days', 'Age upon Outcome Days'], dtype='object')\n",
      "\n",
      "\n",
      "Categorical columns: Index(['Pet ID', 'Sex upon Outcome', 'Name', 'Found Location', 'Intake Type',\n",
      "       'Intake Condition', 'Pet Type', 'Sex upon Intake', 'Breed', 'Color'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "numerical_features_all = df[model_features].select_dtypes(include=np.number).columns\n",
    "print('Numerical columns:',numerical_features_all)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "categorical_features_all = df[model_features].select_dtypes(include='object').columns\n",
    "print('Categorical columns:',categorical_features_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target distribution\n",
    "\n",
    "Let's check our target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df[model_target].value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the target plots we can identify whether or not we are dealing with imbalanced datasets - this means one result type is dominating the other one(s). \n",
    "\n",
    "Handling class imbalance is highly recommended, as the model performance can be greatly impacted. In particular the model may not work well for the infrequent classes, as there are not enough samples to learn patterns from, and so it would be hard for the classifier to identify and match those patterns. \n",
    "\n",
    "We might want to downsample the dominant class or upsample the rare the class, to help with learning its patterns. However, we should only fix the imbalance in training set, without changing the validation and test sets, as these should follow the original distribution. We will perform this task after train/test split. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Select features to build the model</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "This time we build a model using all features. That is, we build a classifier including __numerical, categorical__ and __text__ features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab model features/inputs and target/output\n",
    "\n",
    "# can also grab less numerical features, as some numerical data might not be very useful\n",
    "numerical_features = ['Age upon Intake Days', 'Age upon Outcome Days']\n",
    "\n",
    "# dropping the IDs features, RescuerID and PetID here \n",
    "categorical_features = ['Sex upon Outcome', 'Intake Type',\n",
    "       'Intake Condition', 'Pet Type', 'Sex upon Intake']\n",
    "\n",
    "# from EDA, select the text features\n",
    "text_features = ['Name', 'Found Location', 'Breed', 'Color']\n",
    "    \n",
    "model_features = numerical_features + categorical_features + text_features\n",
    "model_target = 'Outcome Type'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning numerical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age upon Intake Days\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age upon Outcome Days\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for c in numerical_features:\n",
    "    print(c)\n",
    "    df[c].plot.hist(bins=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some histograms the values are heavily placed in the first bin, it is good to check for outliers, either checking the min-max values of those particular features and/or explore value ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age upon Intake Days\n",
      "min: 0 max: 9125\n",
      "Age upon Outcome Days\n",
      "min: 0 max: 9125\n"
     ]
    }
   ],
   "source": [
    "for c in numerical_features:\n",
    "    print(c)\n",
    "    print('min:', df[c].min(), 'max:', df[c].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With __value_counts()__ function, we can increase the number of histogram bins to 10 for more bins for a more refined view of the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age upon Intake Days\n",
      "(-9.126, 912.5]     74835\n",
      "(912.5, 1825.0]     10647\n",
      "(1825.0, 2737.5]     3471\n",
      "(2737.5, 3650.0]     3998\n",
      "(3650.0, 4562.5]     1234\n",
      "(4562.5, 5475.0]     1031\n",
      "(5475.0, 6387.5]      183\n",
      "(6387.5, 7300.0]       79\n",
      "(7300.0, 8212.5]        5\n",
      "(8212.5, 9125.0]        2\n",
      "Name: Age upon Intake Days, dtype: int64\n",
      "\n",
      "\n",
      "Age upon Outcome Days\n",
      "(-9.126, 912.5]     74642\n",
      "(912.5, 1825.0]     10699\n",
      "(1825.0, 2737.5]     3465\n",
      "(2737.5, 3650.0]     4080\n",
      "(3650.0, 4562.5]     1263\n",
      "(4562.5, 5475.0]     1061\n",
      "(5475.0, 6387.5]      187\n",
      "(6387.5, 7300.0]       81\n",
      "(7300.0, 8212.5]        5\n",
      "(8212.5, 9125.0]        2\n",
      "Name: Age upon Outcome Days, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in numerical_features: \n",
    "    print(c)\n",
    "    print(df[c].value_counts(bins=10, sort=False))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any outliers are identified as very likely wrong values, dropping them could improve the numerical values histograms, and later overall model performance. While a good rule of thumb is that anything not in the range of `(Q1 - 1.5 IQR) and (Q3 + 1.5 IQR)` is an outlier, other rules for removing 'outliers' should be considered as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check missing values for these numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age upon Intake Days     0\n",
      "Age upon Outcome Days    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[numerical_features].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any missing values, as a quick fix, we can apply mean imputation. This will replace the missing values with the mean value of the corresponding column.\n",
    "\n",
    "__Note__: The statistically correct way to perform mean/mode imputation before training an ML model is to compute the column-wise means on the training data only, and then use these values to impute missing data in both the train and test sets. So, you'll need to split your dataset first. Same goes for any other transformations we would like to apply to these numerical features, such as scaling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning categorical features \n",
    "\n",
    "Let's also examine the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex upon Outcome\n",
      "['Neutered Male' 'Intact Male' 'Intact Female' 'Unknown' 'Spayed Female'\n",
      " nan]\n",
      "\n",
      "\n",
      "Intake Type\n",
      "['Owner Surrender' 'Stray' 'Wildlife' 'Public Assist' 'Euthanasia Request'\n",
      " 'Abandoned']\n",
      "\n",
      "\n",
      "Intake Condition\n",
      "['Normal' 'Nursing' 'Sick' 'Injured' 'Aged' 'Feral' 'Pregnant' 'Other'\n",
      " 'Behavior' 'Medical']\n",
      "\n",
      "\n",
      "Pet Type\n",
      "['Cat' 'Dog' 'Other' 'Bird' 'Livestock']\n",
      "\n",
      "\n",
      "Sex upon Intake\n",
      "['Neutered Male' 'Intact Male' 'Intact Female' 'Unknown' 'Spayed Female'\n",
      " nan]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in categorical_features:\n",
    "    print(c)\n",
    "    print(df[c].unique())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note on boolean type features__: Some categories might be of boolean type, like __False__ and __True__. The booleans will raise errors when trying to encode the categoricals with sklearn encoders, none of which accept boolean types. If using pandas get_dummies to one-hot encode the categoricals, there's no need to convert the booleans. However, get_dummies is trickier to use with sklearn's Pipeline and GridSearch. \n",
    "\n",
    "One way to deal with the booleans is to convert them to strings, by using a mask and a map changing only the booleans. Another way to handle the booleans is to convert them to strings by changing the type of all categoricals to 'str'. This will also affect the nans, basically performing imputation of the nans with a 'nans' placeholder value! \n",
    "\n",
    "Applying the type conversion to both categoricals and text features, takes care of the nans in the text fields as well. In case other imputations are planned for the categoricals and/or test fields, notice that the masking shown above leaves the nans unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical_features + text_features] = df[categorical_features + text_features].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a check on missing values for the categorical features (and text features here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex upon Outcome    0\n",
      "Intake Type         0\n",
      "Intake Condition    0\n",
      "Pet Type            0\n",
      "Sex upon Intake     0\n",
      "Name                0\n",
      "Found Location      0\n",
      "Breed               0\n",
      "Color               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[categorical_features + text_features].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting categoricals into useful numerical features will also have to wait until after the train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning text features \n",
    "\n",
    "Also a good idea to look at the text fields. Text cleaning can be performed here, before train/test split, with less code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\n",
      "['Chunk' 'Gizmo' 'nan' '*Donatello' '*Zeus' 'Artemis' '*Birch' '*Liza'\n",
      " 'Star' 'Millie']\n",
      "\n",
      "\n",
      "Found Location\n",
      "['Austin (TX)' '7201 Levander Loop in Austin (TX)'\n",
      " '12034 Research in Austin (TX)' '2300 Waterway Bnd in Austin (TX)'\n",
      " '4701 Staggerbrush Rd in Austin (TX)'\n",
      " '10015 Lake Creek Pkwy in Austin (TX)' '9200 N Plaza Dr in Austin (TX)'\n",
      " 'Hill Croft Drive & Loyola Lane in Austin (TX)'\n",
      " '2607 Garrettson Drive in Austin (TX)'\n",
      " '8711 Johnny Morris in Austin (TX)']\n",
      "\n",
      "\n",
      "Breed\n",
      "['Domestic Shorthair Mix' 'Chihuahua Shorthair Mix' 'Domestic Shorthair'\n",
      " 'Opossum' 'Yorkshire Terrier Mix'\n",
      " 'Jack Russell Terrier/Chihuahua Shorthair' 'Great Pyrenees Mix' 'Bat Mix'\n",
      " 'Australian Cattle Dog Mix' 'Labrador Retriever Mix']\n",
      "\n",
      "\n",
      "Color\n",
      "['Brown Tabby/White' 'White/Brown' 'Orange Tabby' 'Black'\n",
      " 'White/Orange Tabby' 'Blue/White' 'Brown Tabby' 'Gray' 'Calico'\n",
      " 'Brown/Black']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in text_features:\n",
    "    print(c)\n",
    "    print(df[c].unique()[:10])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-use the helper functions from the 'Text processing' notebook above.\n",
    "\n",
    "__Warning__: cleaning stage can take a few minutes, depending on how much text is there to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning:  Name\n",
      "Text cleaning:  Found Location\n",
      "Text cleaning:  Breed\n",
      "Text cleaning:  Color\n"
     ]
    }
   ],
   "source": [
    "# Prepare cleaning functions\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preProcessText(text):\n",
    "    # lowercase and strip leading/trailing white space\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    \n",
    "    # remove extra white space\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def lexiconProcess(text, stop_words, stemmer):\n",
    "    filtered_sentence = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(stemmer.stem(w))\n",
    "    text = \" \".join(filtered_sentence)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def cleanSentence(text, stop_words, stemmer):\n",
    "    return lexiconProcess(preProcessText(text), stop_words, stemmer)\n",
    "\n",
    "# Clean the text features\n",
    "for c in text_features:\n",
    "    print('Text cleaning: ', c)\n",
    "    df[c] = [cleanSentence(item, stop_words, stemmer) for item in df[c].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned text features are ready to be vectorized after the train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: More exploratory data analysis might reveal other important hidden atributes and/or relationships of the model features considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Training and test datasets</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We split our dataset into training (90%) and test (10%) subsets using sklearn's [__train_test_split()__](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.1, shuffle=True, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (85936, 13)\n",
      "Class 0 samples in the training set: 37499\n",
      "Class 1 samples in the training set: 48437\n",
      "Class 0 samples in the test set: 4132\n",
      "Class 1 samples in the test set: 5417\n"
     ]
    }
   ],
   "source": [
    "print('Training set shape:', train_data.shape)\n",
    "\n",
    "print('Class 0 samples in the training set:', sum(train_data[model_target] == 0))\n",
    "print('Class 1 samples in the training set:', sum(train_data[model_target] == 1))\n",
    "\n",
    "print('Class 0 samples in the test set:', sum(test_data[model_target] == 0))\n",
    "print('Class 1 samples in the test set:', sum(test_data[model_target] == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important note:__ We want to fix the imbalance only in training set. We shouldn't change the validation and test sets, as these should follow the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "class_0_no = train_data[train_data[model_target] == 0]\n",
    "class_1_no = train_data[train_data[model_target] == 1]\n",
    "\n",
    "upsampled_class_0_no = class_0_no.sample(n=len(class_1_no), replace=True, random_state=42)\n",
    "\n",
    "train_data = pd.concat([class_1_no, upsampled_class_0_no])\n",
    "train_data = shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (96874, 13)\n",
      "Class 1 samples in the training set: 48437\n",
      "Class 0 samples in the training set: 48437\n"
     ]
    }
   ],
   "source": [
    "print('Training set shape:', train_data.shape)\n",
    "\n",
    "print('Class 1 samples in the training set:', sum(train_data[model_target] == 1))\n",
    "print('Class 0 samples in the training set:', sum(train_data[model_target] == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Data processing with ColumnTransformer</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's build a more complex pipeline today. We first build separate pipelines to handle the numerical, categorical, and text features, and then combine them into a composite pipeline along with an estimator, a [XGBoost Classifier](https://github.com/dmlc/xgboost) here.\n",
    "\n",
    "   * For the numerical features pipeline, the __numerical_processor__ below, we impute missing values with the mean using sklearn's SimpleImputer, followed by a MinMaxScaler (don't have to scale features when using tree-based algorithms, but it's a good idea to see how to use more data transforms). If different processing is desired for different numerical features, different pipelines should be built - just like shown below for the two text features.\n",
    "   \n",
    "   \n",
    "   * In the categoricals pipeline, the __categorical_processor__ below, we impute with a placeholder value (no effect here as we already encoded the 'nan's), and encode with sklearn's OneHotEncoder. If computing memory is an issue, it is a good idea to check categoricals' unique values, to get an estimate of many dummy features will be created by one-hot encoding. Note the __handle_unknown__ parameter that tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation/and or test set that was not present in the initial training set.\n",
    "  \n",
    "   \n",
    "   * And, finally, also with memory usage in mind, we build two more pipelines, one for each of our text features, trying different vocabulary sizes.\n",
    "   \n",
    "The selective preparations of the dataset features are then put together into a collective __ColumnTransformer__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline([\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('num_scaler', MinMaxScaler()) # Shown in case is needed, not a must with Trees\n",
    "                                ])\n",
    "                  \n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline([\n",
    "    ('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Shown in case is needed, no effect here as we already imputed with 'nan' strings\n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore')) # handle_unknown tells it to ignore (rather than throw an error for) any value that was not present in the initial training set.\n",
    "                                ])\n",
    "\n",
    "# Preprocess 1st text feature\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(binary=True, max_features=50))\n",
    "                                ])\n",
    "\n",
    "# Preprocess 2nd text feature (larger vocabulary)\n",
    "text_precessor_1 = Pipeline([\n",
    "    ('text_vect_1', CountVectorizer(binary=True, max_features=150))\n",
    "                                ])\n",
    "\n",
    "# Combine all data preprocessors from above (add more, if you choose to define more!)\n",
    "# For each processor/step specify: a name, the actual process, and finally the features to be processed\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('numerical_pre', numerical_processor, numerical_features),\n",
    "    ('categorical_pre', categorical_processor, categorical_features),\n",
    "    ('text_pre_0', text_processor_0, text_features[0]),\n",
    "    ('text_pre_1', text_precessor_1, text_features[1])\n",
    "                                    ]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a name=\"6\">Train a classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We use Amazon SageMaker XGBoost algorithm to build our classifier. We explain the components common to all Amazon SageMaker's algorithms including uploading data to Amazon S3, training a model, and setting up an endpoint for online inference. \n",
    "\n",
    "### Set up the SageMaker environment\n",
    "\n",
    "Let's start by importing libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from os import path\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload datasets to Amazon S3\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a `CSV` or `recordIO-wrapped-protobuf` format. For this example, we stick with CSV.  \n",
    "\n",
    "So, let's write the data to Amazon S3 in recordio-protobuf format. We first create an io buffer wrapping the data, next we upload it to Amazon S3. Notice that the choice of bucket and prefix should change for different users and different datasets.\n",
    "\n",
    "Amazon SageMaker requires that a `CSV file does not have a header record and that the target variable is in the first column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'XGBoost-demo'\n",
    "key = 'sample-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train data and split intro train and validatuin\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data[model_features],\n",
    "    train_data[model_target],\n",
    "    test_size=0.15,\n",
    "    shuffle=True,\n",
    "    random_state=23,\n",
    ")\n",
    "\n",
    "# Get test data\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Learn the transformation from training set and apply to validation and test\n",
    "X_train = data_preprocessor.fit_transform(X_train).toarray()\n",
    "X_val = data_preprocessor.transform(X_val).toarray()\n",
    "X_test = data_preprocessor.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape =  (82342, 235)\n",
      "train_labels shape =  (82342,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-757420736997/XGBoost-demo/train/sample-data'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"train_features shape = \", X_train.shape)\n",
    "print(\"train_labels shape = \", y_train.shape)\n",
    "\n",
    "train_df = pd.DataFrame(\n",
    "    np.concatenate((y_train.values.reshape(-1, 1), X_train), axis=1)\n",
    ")\n",
    "train_df.to_csv(\"/tmp/train.csv\", header=False, index=False)\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train\", key)\n",
    ").upload_file(\"/tmp/train.csv\")\n",
    "os.remove(\"/tmp/train.csv\")\n",
    "\n",
    "s3_train_data = \"s3://{}/{}/train/{}\".format(bucket, prefix, key)\n",
    "s3_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to provide validation data. This way we can get an test of the performance of the model from the training logs. In order to use this capability let's upload the validation data to Amazon S3 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_features shape =  (14532, 235)\n",
      "validation_labels shape =  (14532,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-757420736997/XGBoost-demo/validation/sample-data'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"validation_features shape = \", X_val.shape)\n",
    "print(\"validation_labels shape = \", y_val.shape)\n",
    "\n",
    "val_df = pd.DataFrame(np.concatenate((y_val.values.reshape(-1, 1), X_val), axis=1))\n",
    "val_df.to_csv(\"/tmp/validation.csv\", header=False, index=False)\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"validation\", key)\n",
    ").upload_file(\"/tmp/validation.csv\")\n",
    "os.remove(\"/tmp/validation.csv\")\n",
    "\n",
    "s3_validation_data = \"s3://{}/{}/validation/{}\".format(bucket, prefix, key)\n",
    "s3_validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the classifier\n",
    "\n",
    "We use the built-in Sagemaker [XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) below. \n",
    "\n",
    "In Amazon SageMaker, model training is done via an object called an __estimator__. When setting up the estimator we specify the location (in Amazon S3) of the training data, the path (again in Amazon S3) to the output directory where the model will be serialized, generic hyper-parameters such as the machine type to use during the training process, and kNN-specific hyper-parameters. Once the estimator is initialized, we can call its __.fit()__ method in order to do the actual training.\n",
    "\n",
    "* __Compute power:__ We will use `train_instance_count` and `train_instance_type` parameters. This example uses `ml.m4.xlarge` resource for training. The instance_type is the machine type that will host the model. We can change the instance type for our needs (for example GPUs for neural networks).\n",
    "* __Model type:__ `objective` is set to __`binary:logistic`__, as we have a classification problem here.\n",
    "* __Hyperparameters:__ We update some of the parameters __`max_depth`__, __`num_rounds`__ and __`subsample`__. The __`subsample`__ parameter determines what percentage of randomly sampled points from the data set should be used for building the individual trees. Using all the data points is tempting and definitely can't hurt the quality of the outputs but is often either infeasible or simply too costly. The __`num_round`__ parameter determines how many rounds to use for boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set an output path where the trained model will be saved\n",
    "output_path = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "\n",
    "# set required XGboost hyperparameters\n",
    "hyperparams = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"verbosity\": \"1\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"50\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Create a container with XGBoost\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"latest\")\n",
    "\n",
    "# Call the XGBoost estimator object\n",
    "XGBoost_estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    get_execution_role(),\n",
    "    hyperparameters=hyperparams,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because were training with the CSV file format, well create TrainingInputs that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# define the types of files\n",
    "c_type = \"csv\"\n",
    "\n",
    "train_input = TrainingInput(s3_train_data, content_type=c_type)\n",
    "validation_input = TrainingInput(s3_validation_data, content_type=c_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-30 20:21:25 Starting - Starting the training job...\n",
      "2021-06-30 20:21:27 Starting - Launching requested ML instancesProfilerReport-1625084484: InProgress\n",
      "......\n",
      "2021-06-30 20:22:52 Starting - Preparing the instances for training......\n",
      "2021-06-30 20:23:52 Downloading - Downloading input data...\n",
      "2021-06-30 20:24:24 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-06-30:20:24:25:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-06-30:20:24:25:INFO] File size need to be processed in the node: 89.0mb. Available memory size in the node: 23457.37mb\u001b[0m\n",
      "\u001b[34m[2021-06-30:20:24:25:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[20:24:25] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[20:24:25] 82342x235 matrix with 19350370 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2021-06-30:20:24:25:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[20:24:25] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[20:24:25] 14532x235 matrix with 3415020 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[20:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.173739#011validation-error:0.170795\u001b[0m\n",
      "\u001b[34m[20:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.170933#011validation-error:0.167768\u001b[0m\n",
      "\u001b[34m[20:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.166804#011validation-error:0.162607\u001b[0m\n",
      "\u001b[34m[20:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.164254#011validation-error:0.16302\u001b[0m\n",
      "\u001b[34m[20:24:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.161728#011validation-error:0.161712\u001b[0m\n",
      "\u001b[34m[20:24:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.162274#011validation-error:0.16185\u001b[0m\n",
      "\u001b[34m[20:24:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.16242#011validation-error:0.162813\u001b[0m\n",
      "\u001b[34m[20:24:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.161971#011validation-error:0.162744\u001b[0m\n",
      "\u001b[34m[20:24:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.161096#011validation-error:0.160818\u001b[0m\n",
      "\u001b[34m[20:24:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.160404#011validation-error:0.160198\u001b[0m\n",
      "\u001b[34m[20:24:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.160307#011validation-error:0.160542\u001b[0m\n",
      "\u001b[34m[20:24:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.160137#011validation-error:0.160267\u001b[0m\n",
      "\u001b[34m[20:24:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.159967#011validation-error:0.160336\u001b[0m\n",
      "\u001b[34m[20:24:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.158279#011validation-error:0.159441\u001b[0m\n",
      "\u001b[34m[20:24:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.158291#011validation-error:0.158753\u001b[0m\n",
      "\u001b[34m[20:24:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.159335#011validation-error:0.15951\u001b[0m\n",
      "\u001b[34m[20:24:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.15925#011validation-error:0.159097\u001b[0m\n",
      "\u001b[34m[20:24:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.15704#011validation-error:0.157446\u001b[0m\n",
      "\u001b[34m[20:24:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.157089#011validation-error:0.157308\u001b[0m\n",
      "\u001b[34m[20:24:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.157089#011validation-error:0.158134\u001b[0m\n",
      "\u001b[34m[20:24:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.156712#011validation-error:0.157721\u001b[0m\n",
      "\u001b[34m[20:24:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.156372#011validation-error:0.157859\u001b[0m\n",
      "\u001b[34m[20:24:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.15653#011validation-error:0.157927\u001b[0m\n",
      "\u001b[34m[20:24:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.156372#011validation-error:0.158065\u001b[0m\n",
      "\u001b[34m[20:24:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.155947#011validation-error:0.157583\u001b[0m\n",
      "\u001b[34m[20:24:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.155729#011validation-error:0.157102\u001b[0m\n",
      "\u001b[34m[20:24:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.155814#011validation-error:0.157446\u001b[0m\n",
      "\u001b[34m[20:24:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.155498#011validation-error:0.157514\u001b[0m\n",
      "\u001b[34m[20:24:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.155133#011validation-error:0.156895\u001b[0m\n",
      "\u001b[34m[20:24:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.154708#011validation-error:0.156482\u001b[0m\n",
      "\u001b[34m[20:24:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.154381#011validation-error:0.156276\u001b[0m\n",
      "\u001b[34m[20:24:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.154186#011validation-error:0.156276\u001b[0m\n",
      "\u001b[34m[20:24:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.154344#011validation-error:0.15662\u001b[0m\n",
      "\u001b[34m[20:24:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.154113#011validation-error:0.156276\u001b[0m\n",
      "\u001b[34m[20:24:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.153883#011validation-error:0.156001\u001b[0m\n",
      "\u001b[34m[20:24:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.153822#011validation-error:0.155932\u001b[0m\n",
      "\u001b[34m[20:24:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.153798#011validation-error:0.155794\u001b[0m\n",
      "\u001b[34m[20:24:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.153506#011validation-error:0.155932\u001b[0m\n",
      "\u001b[34m[20:24:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.153397#011validation-error:0.155519\u001b[0m\n",
      "\u001b[34m[20:24:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.153239#011validation-error:0.155244\u001b[0m\n",
      "\u001b[34m[20:24:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.15296#011validation-error:0.155656\u001b[0m\n",
      "\u001b[34m[20:24:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.152996#011validation-error:0.155519\u001b[0m\n",
      "\u001b[34m[20:24:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.15319#011validation-error:0.155312\u001b[0m\n",
      "\u001b[34m[20:24:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.15251#011validation-error:0.155106\u001b[0m\n",
      "\u001b[34m[20:24:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.15251#011validation-error:0.155312\u001b[0m\n",
      "\u001b[34m[20:24:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.152109#011validation-error:0.154968\u001b[0m\n",
      "\u001b[34m[20:24:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.152109#011validation-error:0.154968\u001b[0m\n",
      "\u001b[34m[20:24:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.151903#011validation-error:0.154693\u001b[0m\n",
      "\u001b[34m[20:24:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.151806#011validation-error:0.154968\u001b[0m\n",
      "\u001b[34m[20:24:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.151818#011validation-error:0.154831\u001b[0m\n",
      "\n",
      "2021-06-30 20:24:52 Uploading - Uploading generated training model\n",
      "2021-06-30 20:24:52 Completed - Training job completed\n",
      "Training seconds: 53\n",
      "Billable seconds: 53\n",
      "CPU times: user 476 ms, sys: 17.5 ms, total: 493 ms\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "XGBoost_estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <a name=\"7\">Deploy and test the classifier using SageMaker</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Now we test the best model with the best parameters on \"unseen\" data (our test data).\n",
    "\n",
    "Before that, let's first see how the model works on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:error</td>\n",
       "      <td>0.154421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  metric_name     value\n",
       "0        0.0  train:error  0.154421"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(XGBoost_estimator._current_job_name, \n",
    "                                         metric_names = ['train:error']\n",
    "                                        ).dataframe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If happy with the performance, it is time to deploy the model to another instance of our choice. We set up what is called an __endpoint__. An endpoint is a web service that given a request containing an unlabeled data point, or mini-batch of data points, returns a prediction(s). This allow us to use this model in production environment. \n",
    "\n",
    "Deployed endpoints can be used with other AWS Services such as Lambda and API Gateway. A nice walkthrough is available here: https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/ if you are interested.\n",
    "\n",
    "We use a `ml.t2.medium` instance here, but can also use other instance types such as:, `ml.c4.xlarge` etc. \n",
    "\n",
    "__Warning: This process takes about 10-11 minutes to complete.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------!CPU times: user 402 ms, sys: 17.9 ms, total: 420 ms\n",
      "Wall time: 12min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "compiled_model = XGBoost_estimator\n",
    "compiled_model.name = \"deployed-xgboost-model\"\n",
    "endpoint = \"model-test-xgboost-%s\"%str(bucket.split('-')[-1])\n",
    "\n",
    "XGBoost_predictor = compiled_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.t2.medium\",\n",
    "    serializer=CSVSerializer(),\n",
    "    endpoint_name=endpoint \n",
    ")  # endpoint_name needs to be unique! Use AWS account number as part of name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make some predictions; let's test with 500 points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = XGBoost_predictor.predict(X_test[:500,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to array\n",
    "predictions = np.fromstring(y_pred.decode(\"utf-8\"), sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>167</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>18</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions  0.0  1.0\n",
       "actual               \n",
       "0.0          167   45\n",
       "1.0           18  270"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Confusion matrix\n",
    "pd.crosstab(\n",
    "    index=y_test[:500].values,\n",
    "    columns=np.round(predictions),\n",
    "    rownames=[\"actual\"],\n",
    "    colnames=[\"predictions\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you're ready to be done with this notebook, please run the cell below. This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
