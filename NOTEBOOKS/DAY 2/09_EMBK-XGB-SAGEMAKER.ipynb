{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model - SageMaker\n",
    "\n",
    "In this notebook, we build, train a [__XGBoost Classifier__](https://github.com/dmlc/xgboost) to predict the __Outcome Type__ field of our review dataset.\n",
    "\n",
    "\n",
    "1. <a href=\"#1\">Read the dataset</a>\n",
    "2. <a href=\"#2\">Exploratory Data Analysis</a>\n",
    "3. <a href=\"#3\">Select features to build the model</a>\n",
    "4. <a href=\"#4\">Training and test datasets</a>\n",
    "5. <a href=\"#5\">Data processing with ColumnTransformer</a>\n",
    "6. <a href=\"#6\">Train a classifier using SageMaker</a>\n",
    "7. <a href=\"#7\">Deploy and test the classifier using SageMaker</a>\n",
    "\n",
    "__Notes on [AWS SageMaker](https://docs.aws.amazon.com/sagemaker/index.html):__\n",
    "\n",
    "* Fully managed machine learning service, to quickly and easily get you started on building and training machine learning models - we have seen that already! Integrated Jupyter notebook instances, with easy access to data sources for exploration and analysis, abstract away many of the messy infrastructural details needed for hands-on ML - you don't have to manage servers, install libraries/dependencies, etc.!\n",
    "\n",
    "\n",
    "* Apart from building custom machine learning models in SageMaker notebooks, like we did so far, SageMaker also provides a few [built-in common machine learning algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) (check \"SageMaker Examples\" from your SageMaker instance top menu for a complete updated list) that are optimized to run efficiently against extremely large data in a distributed environment. The trained model can then be directly deployed into a production-ready hosted environment for easy access at inference. \n",
    "\n",
    "__Austin Animal Center Dataset__:\n",
    "\n",
    "In this exercise, we are working with pet adoption data from __Austin Animal Center__. We have two datasets that cover intake and outcome of animals. Intake data is available from [here](https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Intakes/wter-evkm) and outcome is from [here](https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Outcomes/9t4d-g238). \n",
    "\n",
    "In order to work with a single table, we joined the intake and outcome tables using the \"Animal ID\" column and created a single __review.csv__ file. We also didn't consider animals with multiple entries to the facility to keep our dataset simple. If you want to see the original datasets and the merged data with multiple entries, they are available under `DATA/review` folder: Austin_Animal_Center_Intakes.csv, Austin_Animal_Center_Outcomes.csv and Austin_Animal_Center_Intakes_Outcomes.csv.\n",
    "\n",
    "__Dataset schema:__ \n",
    "- __Pet ID__ - Unique ID of pet\n",
    "- __Outcome Type__ - State of pet at the time of recording the outcome (0 = not placed, 1 = placed). This is the field to predict.\n",
    "- __Sex upon Outcome__ - Sex of pet at outcome\n",
    "- __Name__ - Name of pet \n",
    "- __Found Location__ - Found location of pet before entered the center\n",
    "- __Intake Type__ - Circumstances bringing the pet to the center\n",
    "- __Intake Condition__ - Health condition of pet when entered the center\n",
    "- __Pet Type__ - Type of pet\n",
    "- __Sex upon Intake__ - Sex of pet when entered the center\n",
    "- __Breed__ - Breed of pet \n",
    "- __Color__ - Color of pet \n",
    "- __Age upon Intake Days__ - Age of pet when entered the center (days)\n",
    "- __Age upon Outcome Days__ - Age of pet at outcome (days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Upgrade dependencies\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Read the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's read the dataset into a dataframe, using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is: (20000, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "  \n",
    "df = pd.read_csv('../../DATA/review/review_dataset.csv')\n",
    "\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Exploratory Data Analysis</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will look at number of rows, columns and some simple statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pet ID</th>\n",
       "      <th>Outcome Type</th>\n",
       "      <th>Sex upon Outcome</th>\n",
       "      <th>Name</th>\n",
       "      <th>Found Location</th>\n",
       "      <th>Intake Type</th>\n",
       "      <th>Intake Condition</th>\n",
       "      <th>Pet Type</th>\n",
       "      <th>Sex upon Intake</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Color</th>\n",
       "      <th>Age upon Intake Days</th>\n",
       "      <th>Age upon Outcome Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57655</th>\n",
       "      <td>A738580</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>*Marco</td>\n",
       "      <td>1000 Byers Ln in Austin (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Injured</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Black</td>\n",
       "      <td>365</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55086</th>\n",
       "      <td>A707351</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>*Falkor</td>\n",
       "      <td>824  Fairfield Drive in Austin (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Female</td>\n",
       "      <td>Siamese Mix</td>\n",
       "      <td>Lynx Point/White</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92318</th>\n",
       "      <td>A683204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>*Spencer</td>\n",
       "      <td>14811 Chicadee in Pflugerville (TX)</td>\n",
       "      <td>Public Assist</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>Domestic Shorthair Mix</td>\n",
       "      <td>Orange Tabby</td>\n",
       "      <td>1095</td>\n",
       "      <td>1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8052</th>\n",
       "      <td>A723534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1411 W Ben White Blvd in Austin (TX)</td>\n",
       "      <td>Wildlife</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Other</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Bat Mix</td>\n",
       "      <td>Brown</td>\n",
       "      <td>365</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23230</th>\n",
       "      <td>A531069</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>Jrewpy</td>\n",
       "      <td>S. 1St &amp; William Cannon in Austin (TX)</td>\n",
       "      <td>Stray</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>Beagle/Basset Hound</td>\n",
       "      <td>Black Brindle/White</td>\n",
       "      <td>2555</td>\n",
       "      <td>2555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Pet ID  Outcome Type Sex upon Outcome      Name  \\\n",
       "57655  A738580           1.0    Neutered Male    *Marco   \n",
       "55086  A707351           1.0    Spayed Female   *Falkor   \n",
       "92318  A683204           0.0    Neutered Male  *Spencer   \n",
       "8052   A723534           0.0          Unknown       NaN   \n",
       "23230  A531069           1.0    Neutered Male    Jrewpy   \n",
       "\n",
       "                               Found Location    Intake Type Intake Condition  \\\n",
       "57655            1000 Byers Ln in Austin (TX)          Stray          Injured   \n",
       "55086     824  Fairfield Drive in Austin (TX)          Stray           Normal   \n",
       "92318     14811 Chicadee in Pflugerville (TX)  Public Assist           Normal   \n",
       "8052     1411 W Ben White Blvd in Austin (TX)       Wildlife           Normal   \n",
       "23230  S. 1St & William Cannon in Austin (TX)          Stray           Normal   \n",
       "\n",
       "      Pet Type Sex upon Intake                   Breed                Color  \\\n",
       "57655      Cat     Intact Male  Domestic Shorthair Mix                Black   \n",
       "55086      Cat   Intact Female             Siamese Mix     Lynx Point/White   \n",
       "92318      Cat     Intact Male  Domestic Shorthair Mix         Orange Tabby   \n",
       "8052     Other         Unknown                 Bat Mix                Brown   \n",
       "23230      Dog   Neutered Male     Beagle/Basset Hound  Black Brindle/White   \n",
       "\n",
       "       Age upon Intake Days  Age upon Outcome Days  \n",
       "57655                   365                    365  \n",
       "55086                    30                     60  \n",
       "92318                  1095                   1095  \n",
       "8052                    365                    365  \n",
       "23230                  2555                   2555  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first five rows\n",
    "# NaN means missing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20000 entries, 57655 to 37656\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Pet ID                 20000 non-null  object \n",
      " 1   Outcome Type           20000 non-null  float64\n",
      " 2   Sex upon Outcome       20000 non-null  object \n",
      " 3   Name                   12404 non-null  object \n",
      " 4   Found Location         20000 non-null  object \n",
      " 5   Intake Type            20000 non-null  object \n",
      " 6   Intake Condition       20000 non-null  object \n",
      " 7   Pet Type               20000 non-null  object \n",
      " 8   Sex upon Intake        20000 non-null  object \n",
      " 9   Breed                  20000 non-null  object \n",
      " 10  Color                  20000 non-null  object \n",
      " 11  Age upon Intake Days   20000 non-null  int64  \n",
      " 12  Age upon Outcome Days  20000 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(10)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Let's see the data types and non-null values for each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outcome Type</th>\n",
       "      <th>Age upon Intake Days</th>\n",
       "      <th>Age upon Outcome Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.56195</td>\n",
       "      <td>713.660850</td>\n",
       "      <td>727.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.49616</td>\n",
       "      <td>1070.283295</td>\n",
       "      <td>1073.303704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>730.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>7300.000000</td>\n",
       "      <td>7300.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Outcome Type  Age upon Intake Days  Age upon Outcome Days\n",
       "count   20000.00000          20000.000000           20000.000000\n",
       "mean        0.56195            713.660850             727.808400\n",
       "std         0.49616           1070.283295            1073.303704\n",
       "min         0.00000              0.000000               0.000000\n",
       "25%         0.00000             30.000000              60.000000\n",
       "50%         1.00000            365.000000             365.000000\n",
       "75%         1.00000            730.000000             730.000000\n",
       "max         1.00000           7300.000000            7300.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This prints basic statistics for numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate model features and model target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Pet ID', 'Outcome Type', 'Sex upon Outcome', 'Name', 'Found Location',\n",
      "       'Intake Type', 'Intake Condition', 'Pet Type', 'Sex upon Intake',\n",
      "       'Breed', 'Color', 'Age upon Intake Days', 'Age upon Outcome Days'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model features:  Index(['Pet ID', 'Sex upon Outcome', 'Name', 'Found Location', 'Intake Type',\n",
      "       'Intake Condition', 'Pet Type', 'Sex upon Intake', 'Breed', 'Color',\n",
      "       'Age upon Intake Days', 'Age upon Outcome Days'],\n",
      "      dtype='object')\n",
      "Model target:  Outcome Type\n"
     ]
    }
   ],
   "source": [
    "model_features = df.columns.drop('Outcome Type')\n",
    "model_target = 'Outcome Type'\n",
    "\n",
    "print('Model features: ', model_features)\n",
    "print('Model target: ', model_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the features set further, figuring out first what features are numerical or categorical. Beware that some integer-valued features could actually be categorical features, and some categorical features could be text features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: Index(['Age upon Intake Days', 'Age upon Outcome Days'], dtype='object')\n",
      "\n",
      "\n",
      "Categorical columns: Index(['Pet ID', 'Sex upon Outcome', 'Name', 'Found Location', 'Intake Type',\n",
      "       'Intake Condition', 'Pet Type', 'Sex upon Intake', 'Breed', 'Color'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "numerical_features_all = df[model_features].select_dtypes(include=np.number).columns\n",
    "print('Numerical columns:',numerical_features_all)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "categorical_features_all = df[model_features].select_dtypes(include='object').columns\n",
    "print('Categorical columns:',categorical_features_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target distribution\n",
    "\n",
    "Let's check our target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df[model_target].value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the target plots we can identify whether or not we are dealing with imbalanced datasets - this means one result type is dominating the other one(s). \n",
    "\n",
    "Handling class imbalance is highly recommended, as the model performance can be greatly impacted. In particular the model may not work well for the infrequent classes, as there are not enough samples to learn patterns from, and so it would be hard for the classifier to identify and match those patterns. \n",
    "\n",
    "We might want to downsample the dominant class or upsample the rare the class, to help with learning its patterns. However, we should only fix the imbalance in training set, without changing the validation and test sets, as these should follow the original distribution. We will perform this task after train/test split. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Select features to build the model</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "This time we build a model using all features. That is, we build a classifier including __numerical, categorical__ and __text__ features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab model features/inputs and target/output\n",
    "\n",
    "# can also grab less numerical features, as some numerical data might not be very useful\n",
    "numerical_features = ['Age upon Intake Days', 'Age upon Outcome Days']\n",
    "\n",
    "# dropping the IDs features, RescuerID and PetID here \n",
    "categorical_features = ['Sex upon Outcome', 'Intake Type',\n",
    "       'Intake Condition', 'Pet Type', 'Sex upon Intake']\n",
    "\n",
    "# from EDA, select the text features\n",
    "text_features = ['Name', 'Found Location', 'Breed', 'Color']\n",
    "    \n",
    "model_features = numerical_features + categorical_features + text_features\n",
    "model_target = 'Outcome Type'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning numerical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age upon Intake Days\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age upon Outcome Days\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for c in numerical_features:\n",
    "    print(c)\n",
    "    df[c].plot.hist(bins=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some histograms the values are heavily placed in the first bin, it is good to check for outliers, either checking the min-max values of those particular features and/or explore value ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age upon Intake Days\n",
      "min: 0 max: 7300\n",
      "Age upon Outcome Days\n",
      "min: 0 max: 7300\n"
     ]
    }
   ],
   "source": [
    "for c in numerical_features:\n",
    "    print(c)\n",
    "    print('min:', df[c].min(), 'max:', df[c].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With __value_counts()__ function, we can increase the number of histogram bins to 10 for more bins for a more refined view of the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age upon Intake Days\n",
      "(-7.301, 730.0]     15660\n",
      "(730.0, 1460.0]      1629\n",
      "(1460.0, 2190.0]      938\n",
      "(2190.0, 2920.0]      695\n",
      "(2920.0, 3650.0]      519\n",
      "(3650.0, 4380.0]      273\n",
      "(4380.0, 5110.0]      165\n",
      "(5110.0, 5840.0]       87\n",
      "(5840.0, 6570.0]       22\n",
      "(6570.0, 7300.0]       12\n",
      "Name: Age upon Intake Days, dtype: int64\n",
      "\n",
      "\n",
      "Age upon Outcome Days\n",
      "(-7.301, 730.0]     15623\n",
      "(730.0, 1460.0]      1628\n",
      "(1460.0, 2190.0]      947\n",
      "(2190.0, 2920.0]      707\n",
      "(2920.0, 3650.0]      519\n",
      "(3650.0, 4380.0]      281\n",
      "(4380.0, 5110.0]      173\n",
      "(5110.0, 5840.0]       87\n",
      "(5840.0, 6570.0]       23\n",
      "(6570.0, 7300.0]       12\n",
      "Name: Age upon Outcome Days, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in numerical_features: \n",
    "    print(c)\n",
    "    print(df[c].value_counts(bins=10, sort=False))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any outliers are identified as very likely wrong values, dropping them could improve the numerical values histograms, and later overall model performance. While a good rule of thumb is that anything not in the range of `(Q1 - 1.5 IQR) and (Q3 + 1.5 IQR)` is an outlier, other rules for removing 'outliers' should be considered as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check missing values for these numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age upon Intake Days     0\n",
      "Age upon Outcome Days    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[numerical_features].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any missing values, as a quick fix, we can apply mean imputation. This will replace the missing values with the mean value of the corresponding column.\n",
    "\n",
    "__Note__: The statistically correct way to perform mean/mode imputation before training an ML model is to compute the column-wise means on the training data only, and then use these values to impute missing data in both the train and test sets. So, you'll need to split your dataset first. Same goes for any other transformations we would like to apply to these numerical features, such as scaling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning categorical features \n",
    "\n",
    "Let's also examine the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex upon Outcome\n",
      "['Neutered Male' 'Spayed Female' 'Unknown' 'Intact Female' 'Intact Male']\n",
      "\n",
      "\n",
      "Intake Type\n",
      "['Stray' 'Public Assist' 'Wildlife' 'Owner Surrender' 'Euthanasia Request'\n",
      " 'Abandoned']\n",
      "\n",
      "\n",
      "Intake Condition\n",
      "['Injured' 'Normal' 'Nursing' 'Aged' 'Sick' 'Other' 'Feral' 'Medical'\n",
      " 'Pregnant']\n",
      "\n",
      "\n",
      "Pet Type\n",
      "['Cat' 'Other' 'Dog' 'Bird' 'Livestock']\n",
      "\n",
      "\n",
      "Sex upon Intake\n",
      "['Intact Male' 'Intact Female' 'Unknown' 'Neutered Male' 'Spayed Female']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in categorical_features:\n",
    "    print(c)\n",
    "    print(df[c].unique())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note on boolean type features__: Some categories might be of boolean type, like __False__ and __True__. The booleans will raise errors when trying to encode the categoricals with sklearn encoders, none of which accept boolean types. If using pandas get_dummies to one-hot encode the categoricals, there's no need to convert the booleans. However, get_dummies is trickier to use with sklearn's Pipeline and GridSearch. \n",
    "\n",
    "One way to deal with the booleans is to convert them to strings, by using a mask and a map changing only the booleans. Another way to handle the booleans is to convert them to strings by changing the type of all categoricals to 'str'. This will also affect the nans, basically performing imputation of the nans with a 'nans' placeholder value! \n",
    "\n",
    "Applying the type conversion to both categoricals and text features, takes care of the nans in the text fields as well. In case other imputations are planned for the categoricals and/or test fields, notice that the masking shown above leaves the nans unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical_features + text_features] = df[categorical_features + text_features].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a check on missing values for the categorical features (and text features here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex upon Outcome    0\n",
      "Intake Type         0\n",
      "Intake Condition    0\n",
      "Pet Type            0\n",
      "Sex upon Intake     0\n",
      "Name                0\n",
      "Found Location      0\n",
      "Breed               0\n",
      "Color               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[categorical_features + text_features].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting categoricals into useful numerical features will also have to wait until after the train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning text features \n",
    "\n",
    "Also a good idea to look at the text fields. Text cleaning can be performed here, before train/test split, with less code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name\n",
      "['*Marco' '*Falkor' '*Spencer' 'nan' 'Jrewpy' 'R-Two' 'Nayla' 'Axis'\n",
      " 'Anemona' 'Archie']\n",
      "\n",
      "\n",
      "Found Location\n",
      "['1000 Byers Ln in Austin (TX)' '824  Fairfield Drive in Austin (TX)'\n",
      " '14811 Chicadee in Pflugerville (TX)'\n",
      " '1411 W Ben White Blvd in Austin (TX)'\n",
      " 'S. 1St & William Cannon in Austin (TX)' 'Leander (TX)'\n",
      " '9418 Hunters Trace in Austin (TX)' '3212 Red River in Austin (TX)'\n",
      " 'Austin (TX)' 'Travis (TX)']\n",
      "\n",
      "\n",
      "Breed\n",
      "['Domestic Shorthair Mix' 'Siamese Mix' 'Bat Mix' 'Beagle/Basset Hound'\n",
      " 'Pit Bull Mix' 'Siberian Husky' 'Anatol Shepherd Mix'\n",
      " 'Chihuahua Shorthair/Pug' 'Labrador Retriever Mix' 'Domestic Shorthair']\n",
      "\n",
      "\n",
      "Color\n",
      "['Black' 'Lynx Point/White' 'Orange Tabby' 'Brown' 'Black Brindle/White'\n",
      " 'Chocolate/White' 'White' 'Brown Tabby' 'Blue Tabby' 'Tan']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in text_features:\n",
    "    print(c)\n",
    "    print(df[c].unique()[:10])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-use the helper functions from the 'Text processing' notebook above.\n",
    "\n",
    "__Warning__: cleaning stage can take a few minutes, depending on how much text is there to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning:  Name\n",
      "Text cleaning:  Found Location\n",
      "Text cleaning:  Breed\n",
      "Text cleaning:  Color\n"
     ]
    }
   ],
   "source": [
    "# Prepare cleaning functions\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preProcessText(text):\n",
    "    # lowercase and strip leading/trailing white space\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    \n",
    "    # remove extra white space\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def lexiconProcess(text, stop_words, stemmer):\n",
    "    filtered_sentence = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(stemmer.stem(w))\n",
    "    text = \" \".join(filtered_sentence)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def cleanSentence(text, stop_words, stemmer):\n",
    "    return lexiconProcess(preProcessText(text), stop_words, stemmer)\n",
    "\n",
    "# Clean the text features\n",
    "for c in text_features:\n",
    "    print('Text cleaning: ', c)\n",
    "    df[c] = [cleanSentence(item, stop_words, stemmer) for item in df[c].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned text features are ready to be vectorized after the train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: More exploratory data analysis might reveal other important hidden atributes and/or relationships of the model features considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Training and test datasets</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We split our dataset into training (90%) and test (10%) subsets using sklearn's [__train_test_split()__](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.1, shuffle=True, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (18000, 13)\n",
      "Class 0 samples in the training set: 7893\n",
      "Class 1 samples in the training set: 10107\n",
      "Class 0 samples in the test set: 868\n",
      "Class 1 samples in the test set: 1132\n"
     ]
    }
   ],
   "source": [
    "print('Training set shape:', train_data.shape)\n",
    "\n",
    "print('Class 0 samples in the training set:', sum(train_data[model_target] == 0))\n",
    "print('Class 1 samples in the training set:', sum(train_data[model_target] == 1))\n",
    "\n",
    "print('Class 0 samples in the test set:', sum(test_data[model_target] == 0))\n",
    "print('Class 1 samples in the test set:', sum(test_data[model_target] == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important note:__ We want to fix the imbalance only in training set. We shouldn't change the validation and test sets, as these should follow the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "class_0_no = train_data[train_data[model_target] == 0]\n",
    "class_1_no = train_data[train_data[model_target] == 1]\n",
    "\n",
    "upsampled_class_0_no = class_0_no.sample(n=len(class_1_no), replace=True, random_state=42)\n",
    "\n",
    "train_data = pd.concat([class_1_no, upsampled_class_0_no])\n",
    "train_data = shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (20214, 13)\n",
      "Class 1 samples in the training set: 10107\n",
      "Class 0 samples in the training set: 10107\n"
     ]
    }
   ],
   "source": [
    "print('Training set shape:', train_data.shape)\n",
    "\n",
    "print('Class 1 samples in the training set:', sum(train_data[model_target] == 1))\n",
    "print('Class 0 samples in the training set:', sum(train_data[model_target] == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Data processing with ColumnTransformer</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's build a more complex pipeline today. We first build separate pipelines to handle the numerical, categorical, and text features, and then combine them into a composite pipeline along with an estimator, a [XGBoost Classifier](https://github.com/dmlc/xgboost) here.\n",
    "\n",
    "   * For the numerical features pipeline, the __numerical_processor__ below, we impute missing values with the mean using sklearn's SimpleImputer, followed by a MinMaxScaler (don't have to scale features when using tree-based algorithms, but it's a good idea to see how to use more data transforms). If different processing is desired for different numerical features, different pipelines should be built - just like shown below for the two text features.\n",
    "   \n",
    "   \n",
    "   * In the categoricals pipeline, the __categorical_processor__ below, we impute with a placeholder value (no effect here as we already encoded the 'nan's), and encode with sklearn's OneHotEncoder. If computing memory is an issue, it is a good idea to check categoricals' unique values, to get an estimate of many dummy features will be created by one-hot encoding. Note the __handle_unknown__ parameter that tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation/and or test set that was not present in the initial training set.\n",
    "  \n",
    "   \n",
    "   * And, finally, also with memory usage in mind, we build two more pipelines, one for each of our text features, trying different vocabulary sizes.\n",
    "   \n",
    "The selective preparations of the dataset features are then put together into a collective __ColumnTransformer__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline([\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('num_scaler', MinMaxScaler()) # Shown in case is needed, not a must with Trees\n",
    "                                ])\n",
    "                  \n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline([\n",
    "    ('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Shown in case is needed, no effect here as we already imputed with 'nan' strings\n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore')) # handle_unknown tells it to ignore (rather than throw an error for) any value that was not present in the initial training set.\n",
    "                                ])\n",
    "\n",
    "# Preprocess 1st text feature\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(binary=True, max_features=50))\n",
    "                                ])\n",
    "\n",
    "# Preprocess 2nd text feature (larger vocabulary)\n",
    "text_precessor_1 = Pipeline([\n",
    "    ('text_vect_1', CountVectorizer(binary=True, max_features=150))\n",
    "                                ])\n",
    "\n",
    "# Combine all data preprocessors from above (add more, if you choose to define more!)\n",
    "# For each processor/step specify: a name, the actual process, and finally the features to be processed\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('numerical_pre', numerical_processor, numerical_features),\n",
    "    ('categorical_pre', categorical_processor, categorical_features),\n",
    "    ('text_pre_0', text_processor_0, text_features[0]),\n",
    "    ('text_pre_1', text_precessor_1, text_features[1])\n",
    "                                    ]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a name=\"6\">Train a classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We use Amazon SageMaker XGBoost algorithm to build our classifier. We explain the components common to all Amazon SageMaker's algorithms including uploading data to Amazon S3, training a model, and setting up an endpoint for online inference. \n",
    "\n",
    "### Set up the SageMaker environment\n",
    "\n",
    "Let's start by importing libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from os import path\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload datasets to Amazon S3\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a `CSV` or `recordIO-wrapped-protobuf` format. For this example, we stick with CSV.  \n",
    "\n",
    "So, let's write the data to Amazon S3 in recordio-protobuf format. We first create an io buffer wrapping the data, next we upload it to Amazon S3. Notice that the choice of bucket and prefix should change for different users and different datasets.\n",
    "\n",
    "Amazon SageMaker requires that a `CSV file does not have a header record and that the target variable is in the first column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'XGBoost-demo'\n",
    "key = 'sample-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train data and split intro train and validatuin\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data[model_features],\n",
    "    train_data[model_target],\n",
    "    test_size=0.15,\n",
    "    shuffle=True,\n",
    "    random_state=23,\n",
    ")\n",
    "\n",
    "# Get test data\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Learn the transformation from training set and apply to validation and test\n",
    "X_train = data_preprocessor.fit_transform(X_train).toarray()\n",
    "X_val = data_preprocessor.transform(X_val).toarray()\n",
    "X_test = data_preprocessor.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features shape =  (17181, 232)\n",
      "train_labels shape =  (17181,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"train_features shape = \", X_train.shape)\n",
    "print(\"train_labels shape = \", y_train.shape)\n",
    "\n",
    "train_df = pd.DataFrame(\n",
    "    np.concatenate((y_train.values.reshape(-1, 1), X_train), axis=1)\n",
    ")\n",
    "train_df.to_csv(\"train.csv\", header=False, index=False)\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train\", key)\n",
    ").upload_file(\"train.csv\")\n",
    "os.remove(\"train.csv\")\n",
    "\n",
    "s3_train_data = \"s3://{}/{}/train/{}\".format(bucket, prefix, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to provide validation data. This way we can get an test of the performance of the model from the training logs. In order to use this capability let's upload the validation data to Amazon S3 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_features shape =  (3033, 232)\n",
      "validation_labels shape =  (3033,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"validation_features shape = \", X_val.shape)\n",
    "print(\"validation_labels shape = \", y_val.shape)\n",
    "\n",
    "val_df = pd.DataFrame(np.concatenate((y_val.values.reshape(-1, 1), X_val), axis=1))\n",
    "val_df.to_csv(\"validation.csv\", header=False, index=False)\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"validation\", key)\n",
    ").upload_file(\"validation.csv\")\n",
    "os.remove(\"validation.csv\")\n",
    "\n",
    "s3_validation_data = \"s3://{}/{}/validation/{}\".format(bucket, prefix, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the classifier\n",
    "\n",
    "We use the built-in Sagemaker [XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) below. \n",
    "\n",
    "In Amazon SageMaker, model training is done via an object called an __estimator__. When setting up the estimator we specify the location (in Amazon S3) of the training data, the path (again in Amazon S3) to the output directory where the model will be serialized, generic hyper-parameters such as the machine type to use during the training process, and kNN-specific hyper-parameters. Once the estimator is initialized, we can call its __.fit()__ method in order to do the actual training.\n",
    "\n",
    "* __Compute power:__ We will use `train_instance_count` and `train_instance_type` parameters. This example uses `ml.m4.xlarge` resource for training. The instance_type is the machine type that will host the model. We can change the instance type for our needs (for example GPUs for neural networks).\n",
    "* __Model type:__ `objective` is set to __`binary:logistic`__, as we have a classification problem here.\n",
    "* __Hyperparameters:__ We update some of the parameters __`max_depth`__, __`num_rounds`__ and __`subsample`__. The __`subsample`__ parameter determines what percentage of randomly sampled points from the data set should be used for building the individual trees. Using all the data points is tempting and definitely can't hurt the quality of the outputs but is often either infeasible or simply too costly. The __`num_round`__ parameter determines how many rounds to use for boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set an output path where the trained model will be saved\n",
    "output_path = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "\n",
    "# set required XGboost hyperparameters\n",
    "hyperparams = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"verbosity\": \"1\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"50\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Create a container with XGBoost\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"latest\")\n",
    "\n",
    "# Call the XGBoost estimator object\n",
    "XGBoost_estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    get_execution_role(),\n",
    "    hyperparameters=hyperparams,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we’re training with the CSV file format, we’ll create TrainingInputs that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# define the types of files\n",
    "c_type = \"csv\"\n",
    "\n",
    "train_input = TrainingInput(s3_train_data, content_type=c_type)\n",
    "validation_input = TrainingInput(s3_validation_data, content_type=c_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-28 04:07:53 Starting - Starting the training job...\n",
      "2021-05-28 04:08:16 Starting - Launching requested ML instancesProfilerReport-1622174872: InProgress\n",
      ".........\n",
      "2021-05-28 04:09:37 Starting - Preparing the instances for training...\n",
      "2021-05-28 04:10:18 Downloading - Downloading input data...\n",
      "2021-05-28 04:10:50 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-05-28:04:10:51:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-05-28:04:10:51:INFO] File size need to be processed in the node: 18.54mb. Available memory size in the node: 23808.36mb\u001b[0m\n",
      "\u001b[34m[2021-05-28:04:10:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[04:10:51] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[04:10:51] 17181x232 matrix with 3985992 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2021-05-28:04:10:51:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[04:10:51] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[04:10:51] 3033x232 matrix with 703656 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.170013#011validation-error:0.171447\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.162098#011validation-error:0.167161\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.163611#011validation-error:0.16848\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.162505#011validation-error:0.169139\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.159537#011validation-error:0.165842\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.158314#011validation-error:0.164524\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.158082#011validation-error:0.165842\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.157209#011validation-error:0.164524\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.157267#011validation-error:0.165842\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.157034#011validation-error:0.165842\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.156627#011validation-error:0.165842\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.156335#011validation-error:0.165513\u001b[0m\n",
      "\u001b[34m[04:10:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.156044#011validation-error:0.165183\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.15488#011validation-error:0.163864\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.154298#011validation-error:0.160567\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.154357#011validation-error:0.160567\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.154298#011validation-error:0.161227\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.153716#011validation-error:0.161556\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.153251#011validation-error:0.161227\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.152669#011validation-error:0.160897\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.152145#011validation-error:0.161227\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.152028#011validation-error:0.161227\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.152028#011validation-error:0.161227\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.151737#011validation-error:0.161227\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.151039#011validation-error:0.160567\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.150864#011validation-error:0.160237\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.150748#011validation-error:0.159908\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.150864#011validation-error:0.159578\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.151039#011validation-error:0.159578\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.150748#011validation-error:0.160567\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.149758#011validation-error:0.159248\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.148769#011validation-error:0.15694\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.148245#011validation-error:0.156611\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.147838#011validation-error:0.158259\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.148187#011validation-error:0.15727\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.148012#011validation-error:0.158919\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.147838#011validation-error:0.159248\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.14842#011validation-error:0.159248\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.148071#011validation-error:0.159248\u001b[0m\n",
      "\u001b[34m[04:10:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.14778#011validation-error:0.159248\u001b[0m\n",
      "\u001b[34m[04:10:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.147197#011validation-error:0.160897\u001b[0m\n",
      "\u001b[34m[04:10:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.146266#011validation-error:0.159908\u001b[0m\n",
      "\u001b[34m[04:10:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.146499#011validation-error:0.160897\u001b[0m\n",
      "\u001b[34m[04:10:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.14615#011validation-error:0.161227\u001b[0m\n",
      "\u001b[34m[04:10:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.145568#011validation-error:0.160897\u001b[0m\n",
      "\u001b[34m[04:10:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.145219#011validation-error:0.159908\u001b[0m\n",
      "\u001b[34m[04:10:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.145277#011validation-error:0.159248\u001b[0m\n",
      "\u001b[34m[04:10:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.14516#011validation-error:0.158919\u001b[0m\n",
      "\u001b[34m[04:10:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.144811#011validation-error:0.160237\u001b[0m\n",
      "\u001b[34m[04:10:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.144345#011validation-error:0.160897\u001b[0m\n",
      "\n",
      "2021-05-28 04:11:17 Uploading - Uploading generated training model\n",
      "2021-05-28 04:11:17 Completed - Training job completed\n",
      "Training seconds: 46\n",
      "Billable seconds: 46\n",
      "CPU times: user 437 ms, sys: 20.8 ms, total: 458 ms\n",
      "Wall time: 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "XGBoost_estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <a name=\"7\">Deploy and test the classifier using SageMaker</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Now we test the best model with the best parameters on \"unseen\" data (our test data).\n",
    "\n",
    "Before that, let's first see how the model works on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:error</td>\n",
       "      <td>0.148186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  metric_name     value\n",
       "0        0.0  train:error  0.148186"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(XGBoost_estimator._current_job_name, \n",
    "                                         metric_names = ['train:error']\n",
    "                                        ).dataframe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If happy with the performance, it is time to deploy the model to another instance of our choice. We set up what is called an __endpoint__. An endpoint is a web service that given a request containing an unlabeled data point, or mini-batch of data points, returns a prediction(s). This allow us to use this model in production environment. \n",
    "\n",
    "Deployed endpoints can be used with other AWS Services such as Lambda and API Gateway. A nice walkthrough is available here: https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/ if you are interested.\n",
    "\n",
    "We use a `ml.t2.medium` instance here, but can also use other instance types such as:, `ml.c4.xlarge` etc. \n",
    "\n",
    "__Warning: This process takes about 10-11 minutes to complete.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!CPU times: user 319 ms, sys: 7.39 ms, total: 326 ms\n",
      "Wall time: 9min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "compiled_model = XGBoost_estimator\n",
    "compiled_model.name = \"deployed-xgboost-model\"\n",
    "\n",
    "XGBoost_predictor = compiled_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.t2.medium\",\n",
    "    serializer=CSVSerializer(),\n",
    "    endpoint_name=\"model-test-xgboost-123456789\",\n",
    ")  # endpoint_name needs to be unique!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = XGBoost_predictor.predict(X_test[:500,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to array\n",
    "predictions = np.fromstring(y_pred.decode(\"utf-8\"), sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>146</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>22</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions  0.0  1.0\n",
       "actual               \n",
       "0.0          146   57\n",
       "1.0           22  275"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Confusion matrix\n",
    "pd.crosstab(\n",
    "    index=y_test[:500].values,\n",
    "    columns=np.round(predictions),\n",
    "    rownames=[\"actual\"],\n",
    "    colnames=[\"predictions\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you're ready to be done with this notebook, please run the cell below. This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
